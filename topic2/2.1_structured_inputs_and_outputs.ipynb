{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexUmnov/LLM-Engineering-Essentials/blob/main/topic2/2.1_structured_inputs_and_outputs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Engineering Essentials by Nebius Academy\n",
        "\n",
        "Course github: [link](https://github.com/Nebius-Academy/LLM-Engineering-Essentials/tree/main)\n",
        "\n",
        "# 2.1. Structured Inputs and Outputs\n",
        "\n",
        "In Topic 1, we learnt how to prompt an LLM in such a way that it understands what you want from it and gives a relevant answer. In this notebook we'll continue this discussion by understanding\n",
        "\n",
        "* How to make prompts reusable by using **prompt templates**\n",
        "* How to ensure that an LLM creates its outputs in a convenient, easily parsable format\n",
        "\n",
        "Let's start by running some code which will help us in the whole notebook:"
      ],
      "metadata": {
        "id": "XXuQF3YXe0C_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai -qU"
      ],
      "metadata": {
        "id": "vmxtcQ2de0DB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "import os\n",
        "\n",
        "os.environ['NEBIUS_API_KEY'] = userdata.get(\"nebius_api_key\")\n",
        "\n",
        "nebius_client = OpenAI(\n",
        "    base_url=\"https://api.studio.nebius.ai/v1/\",\n",
        "    api_key=os.environ.get(\"NEBIUS_API_KEY\"),\n",
        ")\n",
        "\n",
        "llama_model = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
        "\n",
        "def prettify_string(text, max_line_length=80):\n",
        "    \"\"\"Prints a string with line breaks at spaces to prevent horizontal scrolling.\n",
        "\n",
        "    Args:\n",
        "        text: The string to print.\n",
        "        max_line_length: The maximum length of each line.\n",
        "    \"\"\"\n",
        "\n",
        "    output_lines = []\n",
        "    lines = text.split(\"\\n\")\n",
        "    for line in lines:\n",
        "        current_line = \"\"\n",
        "        words = line.split()\n",
        "        for word in words:\n",
        "            if len(current_line) + len(word) + 1 <= max_line_length:\n",
        "                current_line += word + \" \"\n",
        "            else:\n",
        "                output_lines.append(current_line.strip())\n",
        "                current_line = word + \" \"\n",
        "        output_lines.append(current_line.strip())  # Append the last line\n",
        "    return \"\\n\".join(output_lines)\n",
        "\n",
        "def answer_with_llm(prompt: str,\n",
        "                    system_prompt=\"You are a helpful assistant\",\n",
        "                    max_tokens=512,\n",
        "                    client=nebius_client,\n",
        "                    model=llama_model,\n",
        "                    prettify=True,\n",
        "                    temperature=None) -> str:\n",
        "\n",
        "    messages = []\n",
        "\n",
        "    if system_prompt:\n",
        "        messages.append(\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": system_prompt\n",
        "            }\n",
        "        )\n",
        "\n",
        "    messages.append(\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt\n",
        "        }\n",
        "    )\n",
        "\n",
        "    completion = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        max_tokens=max_tokens,\n",
        "        temperature=temperature\n",
        "    )\n",
        "\n",
        "    if prettify:\n",
        "        return prettify_string(completion.choices[0].message.content)\n",
        "    else:\n",
        "        return completion.choices[0].message.content\n"
      ],
      "metadata": {
        "id": "a1AxEa78e0DB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt templates"
      ],
      "metadata": {
        "id": "L3wUdp21e0DC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In an LLM-powered system, there's always a layer of prompting logic hidden from the user. For example, ChatGPT, Claude, Gemini and others have quite elaborate **system prompts** that set up rules and guardrails of LLM's communication with the user.\n",
        "\n",
        "However, in some cases a system prompting isn't a flexible enough mechanism. Imagine, for example,\n",
        "\n",
        "* a customer support bot that needs to be aware of the user's geography to give relevant answers about locally available products\n",
        "* a railway service support bot that needs to be aware of today's railway strikes and other calamities\n",
        "\n",
        "You'll likely need to insert this information in the middle of the prompt; and for such things, **prompt templates** are a great tool."
      ],
      "metadata": {
        "id": "xeHMXwGEe0DC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basically, a **prompt template** is a template string like\n",
        "\n",
        "```python\n",
        "\"some fixed information {template placeholder 1}\n",
        "some more fixed information {template placeholder 2}\"\n",
        "```\n",
        "\n",
        "where the template placeholders are to be filled in just before an actual LLM call.\n",
        "\n",
        "Let's check several neat ways of wrapping this logic.\n",
        "\n",
        "First of all, you can write your own wrapper. In the example below, `m['content'].format(**kwargs)` allows to put as much formatting as you wish into the user's message."
      ],
      "metadata": {
        "id": "6i-MRNQMZvWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Dict\n",
        "\n",
        "class MessagesPromptTemplate():\n",
        "    messages: List[Dict]\n",
        "\n",
        "    def __init__(self, messages: List[Dict]):\n",
        "        self.messages = messages\n",
        "\n",
        "    def format(self, **kwargs):\n",
        "        return [\n",
        "            {\n",
        "                \"role\":  m['role'],\n",
        "                \"content\": m['content'].format(**kwargs)\n",
        "            }\n",
        "            for m in self.messages\n",
        "        ]"
      ],
      "metadata": {
        "id": "ZCv8sZ5Be0DC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = MessagesPromptTemplate(\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You only answer in rhymes\"},\n",
        "        {\"role\": \"user\", \"content\": \"Tell me about {city}\"}\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "wrNIeKu3e0DD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template.format(city=\"Paris\")"
      ],
      "metadata": {
        "id": "kjL-E-bGe0DD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try calling an llm with different variables"
      ],
      "metadata": {
        "id": "jspgVhL0e0DD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = nebius_client.chat.completions.create(\n",
        "    messages=prompt_template.format(city=\"Paris\"),\n",
        "    model=llama_model\n",
        ").choices[0].message.content\n",
        "print(outputs)"
      ],
      "metadata": {
        "id": "DuR5IqTGe0DD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = nebius_client.chat.completions.create(\n",
        "    messages=prompt_template.format(city=\"Amsterdam\"),\n",
        "    model=llama_model,\n",
        ").choices[0].message.content\n",
        "print(outputs)"
      ],
      "metadata": {
        "id": "_qP1h3ahe0DE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The prompt template class we've written is very primitive and would fail if, for example, some keys aren't inputted.\n",
        "\n",
        "One of the good implementations of prompt templates can be found in LangChain [PromptTemplates](https://python.langchain.com/docs/concepts/prompt_templates/)"
      ],
      "metadata": {
        "id": "3nSqLBC2e0DE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain -qU"
      ],
      "metadata": {
        "id": "m6nkay_qe0DE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_template = ChatPromptTemplate([\n",
        "    (\"system\", \"You only answer in rhymes\"),\n",
        "    (\"user\", \"Tell me about {city}\")\n",
        "])\n",
        "\n",
        "prompt_template.invoke({\"city\": \"Madrid\"})"
      ],
      "metadata": {
        "id": "GeZEj5pCe0DE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** You don't have to use LangChain llm calls or anything else, you can only take their PromptTemplate implementation.\n",
        "\n",
        "However, there's quiet a bit of useful code in that library."
      ],
      "metadata": {
        "id": "5zweywyae0DE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import convert_to_openai_messages"
      ],
      "metadata": {
        "id": "8SyzAiTge0DF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "templated_messages = convert_to_openai_messages(prompt_template.invoke({\"city\": \"Madrid\"}).to_messages())\n",
        "templated_messages"
      ],
      "metadata": {
        "id": "xvE2qTQce0DF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = nebius_client.chat.completions.create(\n",
        "    messages=templated_messages,\n",
        "    model=llama_model,\n",
        ").choices[0].message.content\n",
        "print(outputs)"
      ],
      "metadata": {
        "id": "awT49JQve0DF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Structuring LLM outputs\n",
        "\n",
        "In many cases you require not just a free text answer, but something particular you can use later in your system. For example, if you want your LLM to classify a customet's intent to later pass the conversation to a relevant department, you need to extract the particular intent class from the LLM's answer.\n",
        "\n",
        "To parse your LLM outputs conveniently, it's wise to structure them in a specific way. We've already discussed some prompting tricks in Topic 1; this time, we'll learn several more reliable ways of making the LLM abide a deisgnated output format."
      ],
      "metadata": {
        "id": "PLtqUw9ce0DF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic output structuring\n",
        "\n",
        "As a basic way to structure your output, you can \"ask\" an LLM to present the output in a specific format. For example:"
      ],
      "metadata": {
        "id": "Wb8TpmGZe0DF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = nebius_client.chat.completions.create(\n",
        "    messages=[{\n",
        "        'role': 'user',\n",
        "        'content': \"\"\"Design one role play character\\'s name, class and a short description.\n",
        "Present it as a markdown list\"\"\"}],\n",
        "    model=llama_model,\n",
        ").choices[0].message.content\n",
        "print(outputs)"
      ],
      "metadata": {
        "id": "liGqj3PKe0DF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "While this is quite good, it's not very reliable. A better way would be to show some examples to LLM so that it knows what we expect.\n",
        "\n",
        "These examples are known as **few-shot examples** and the prompting technique itself - as **few-shot prompting**."
      ],
      "metadata": {
        "id": "y7U9nI3Ae0DG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = nebius_client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            'role': 'user',\n",
        "            'content': 'Design one role play character\\'s name, class and a short description. Present it as a markdown list.\\n'\\\n",
        "            \"Examples:\\n\"\\\n",
        "            \"\\n\"\\\n",
        "            \"- **Name:** Randalf the Yellow;\\n\"\\\n",
        "            \"- **Class:** Fire mage;\\n\"\\\n",
        "            \"- **Proficiency:** Pyro magic;\\n\"\\\n",
        "            \"- **Resistance:** Fire;\\n\"\\\n",
        "            \"\\n\"\\\n",
        "            \"- **Name:** Bonan;\\n\"\\\n",
        "            \"- **Class:** Barbarian;\\n\"\\\n",
        "            \"- **Proficiency:** Axe;\\n\"\\\n",
        "            \"- **Resistance:** Mental magic;\\n\"\\\n",
        "        }\n",
        "    ],\n",
        "    model=llama_model,\n",
        ").choices[0].message.content\n",
        "print(outputs)"
      ],
      "metadata": {
        "id": "I5lonLame0DG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, LLM captured the format pretty well.\n"
      ],
      "metadata": {
        "id": "PzygVBe0e0DG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = nebius_client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            'role': 'user',\n",
        "            'content': 'Solve the following equation and output only the answer number without reasoning after \"Answer:\"\\n' \\\n",
        "            '123 * 321 = ?\\n' \\\n",
        "            'Answer:'\n",
        "        }\n",
        "    ],\n",
        "    model=llama_model,\n",
        ").choices[0].message.content\n",
        "print(outputs)"
      ],
      "metadata": {
        "id": "S4KXsxtZe0DG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ever though the answer isn't correct (LLMs are notoriously bad at arithmetics), the output structure is correct and easy to parse out."
      ],
      "metadata": {
        "id": "BtVmakLFe0DG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, we can do even better."
      ],
      "metadata": {
        "id": "JNQ0ns6Oe0DG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Structured outputs\n",
        "\n",
        "Modern LLMs support outputing in a specific format, for example we can use \"JSON mode\" to force outputs to be in JSON fromat."
      ],
      "metadata": {
        "id": "_2G1O-w6e0DG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "json_output = nebius_client.chat.completions.create(\n",
        "    messages=[{'role': 'user', 'content': 'Design a role play character\\'s name, class and a short description in json format'}],\n",
        "    model=llama_model,\n",
        "    response_format={\"type\": \"json_object\"}\n",
        ").choices[0].message.content\n",
        "json_output"
      ],
      "metadata": {
        "id": "Q88QagE-e0DG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is useful, because that'll make it much easier for you later to parse the outputs:"
      ],
      "metadata": {
        "id": "m4KVrmlFe0DH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "json.loads(json_output)"
      ],
      "metadata": {
        "id": "b9-7ouAie0DH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can go another step further and actually define a `pydantic` model to create a schema for our outputs:"
      ],
      "metadata": {
        "id": "tTrOKdoee0DH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from pydantic import BaseModel\n",
        "\n",
        "class CharacterProfile(BaseModel):\n",
        "    name: str\n",
        "    age: int\n",
        "    special_skills: List[str]\n",
        "    traits: List[str]\n",
        "    character_class: str\n",
        "    origin: str\n",
        "\n",
        "completion = nebius_client.chat.completions.create(\n",
        "    model=llama_model,\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Design a role play character\"}\n",
        "    ],\n",
        "    extra_body={\n",
        "        \"guided_json\": CharacterProfile.model_json_schema()\n",
        "    }\n",
        ")\n",
        "\n",
        "CharacterProfile.model_validate_json(completion.choices[0].message.content)"
      ],
      "metadata": {
        "id": "dki1ECRee0DH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So no we have predefined format of outputs, which is easy to work with."
      ],
      "metadata": {
        "id": "2viyoMVOe0DH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another way to structure outputs is using examples\n",
        "\n",
        "Let's consider an example from a famous [MMLU dataset](https://huggingface.co/datasets/cais/mmlu):"
      ],
      "metadata": {
        "id": "TpeDDvNse0DH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Which of the following statements about Ethernets is typically FALSE?\"\n",
        "\n",
        "A = \"Ethernets use circuit switching to send messages.\"\n",
        "B = \"Ethernets use buses with multiple masters.\"\n",
        "C = \"Ethernet protocols use a collision-detection method to ensure that messages are transmitted properly.\"\n",
        "D = \"Networks connected by Ethernets are limited in length to a few hundred meters.\"\n",
        "\n",
        "correct_answer = \"A\""
      ],
      "metadata": {
        "id": "tygKuanMe0DI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ideally we want our LLM to solve this \"test\" by answering to us with a letter corresponding to the right answer. This will also make calculating metrics much easier. Let's see what would happen."
      ],
      "metadata": {
        "id": "s5vvF33Ve0DI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = nebius_client.chat.completions.create(\n",
        "    messages=[{\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"\"\"\n",
        "Answer the following question with one of the options listed below\n",
        "Question: {question}\n",
        "A: {A}\n",
        "B: {B}\n",
        "C: {C}\n",
        "D: {D}\n",
        "Answer:\n",
        "\"\"\"}],\n",
        "    model=llama_model,\n",
        ").choices[0].message.content\n",
        "print(output)"
      ],
      "metadata": {
        "id": "cG2GCODVe0DI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, it did output the right answer, but if we do a simple comparison, we'll get into trouble:"
      ],
      "metadata": {
        "id": "TaJNBEK7e0DI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output == correct_answer"
      ],
      "metadata": {
        "id": "hIadZJfVe0DI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So let's teach our model to answer in the right way using so-called Few Shot Prompting also known as In-Context Learning. We essentially show the model some examples in the prompt to teach it in which format we want the answer to be"
      ],
      "metadata": {
        "id": "v6igGq25e0DI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = nebius_client.chat.completions.create(\n",
        "    messages=[{\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"\"\"\n",
        "Examples:\n",
        "Question: The IP protocol is primarily concerned with\n",
        "A: Routing packets through the network\n",
        "B: Reliable delivery of packets between directly connected machines\n",
        "C: Reliable delivery of large (multi-packet) messages between machines that are not necessarily directly connected\n",
        "D: Dealing with differences among operating system architectures\n",
        "Answer:\n",
        "A\n",
        "\n",
        "Question: Which of the following is NOT a property of bitmap graphics?\n",
        "A: Fast hardware exists to move blocks of pixels efficiently\n",
        "B: Realistic lighting and shading can be done.\n",
        "C: All line segments can be displayed as straight.\n",
        "D: Polygons can be filled with solid colors and textures.\n",
        "Answer:\n",
        "A\n",
        "\n",
        "Task:\n",
        "Answer the following question with one of the options listed below. Only ouput the answer in the same format as the examples.\n",
        "Question: {question}\n",
        "A: {A}\n",
        "B: {B}\n",
        "C: {C}\n",
        "D: {D}\n",
        "Answer:\n",
        "\"\"\"}],\n",
        "    model=llama_model,\n",
        ").choices[0].message.content\n",
        "print(output)"
      ],
      "metadata": {
        "id": "ahOoSsxhe0DI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output == correct_answer"
      ],
      "metadata": {
        "id": "_wlgxkEue0DI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also have observed that for some models the dialog format is actually a better way to structure the Few-Shot examples"
      ],
      "metadata": {
        "id": "uwEVq78Ve0DI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = nebius_client.chat.completions.create(\n",
        "    messages=[{\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"\"\"\n",
        "User: Answer the following question with one of the options listed below.\n",
        "Question: The IP protocol is primarily concerned with\n",
        "A: Routing packets through the network\n",
        "B: Reliable delivery of packets between directly connected machines\n",
        "C: Reliable delivery of large (multi-packet) messages between machines that are not necessarily directly connected\n",
        "D: Dealing with differences among operating system architectures\n",
        "Answer:\n",
        "Assistant: A\n",
        "\n",
        "User: Answer the following question with one of the options listed below.\n",
        "Question: Which of the following is NOT a property of bitmap graphics?\n",
        "A: Fast hardware exists to move blocks of pixels efficiently\n",
        "B: Realistic lighting and shading can be done.\n",
        "C: All line segments can be displayed as straight.\n",
        "D: Polygons can be filled with solid colors and textures.\n",
        "Answer:\n",
        "Assistant: A\n",
        "\n",
        "User: Answer the following question with one of the options listed below.\n",
        "Question: {question}\n",
        "A: {A}\n",
        "B: {B}\n",
        "C: {C}\n",
        "D: {D}\n",
        "Answer:\n",
        "Assitant:\n",
        "\"\"\"}],\n",
        "    model=llama_model,\n",
        ").choices[0].message.content\n",
        "print(output)"
      ],
      "metadata": {
        "id": "WsQW4lNqe0DJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Theoretically we don't even need to show the model relevant examples if we want it to learn the output formatting"
      ],
      "metadata": {
        "id": "rClIT520e0DJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = nebius_client.chat.completions.create(\n",
        "    messages=[{\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"\"\"\n",
        "Question: Choose the letter A\n",
        "A: A\n",
        "B: B\n",
        "C: C\n",
        "D: D\n",
        "Answer:\n",
        "A\n",
        "\n",
        "Question: Which is the biggest number?\n",
        "A: 1\n",
        "B: 2\n",
        "C: 3\n",
        "D: 4\n",
        "Answer:\n",
        "D\n",
        "\n",
        "Answer the following question with one of the options listed below\n",
        "Question: {question}\n",
        "A: {A}\n",
        "B: {B}\n",
        "C: {C}\n",
        "D: {D}\n",
        "Answer:\n",
        "\"\"\"}],\n",
        "    model=llama_model,\n",
        ").choices[0].message.content\n",
        "print(output)"
      ],
      "metadata": {
        "id": "bUyAniI1e0DJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** Sometimes you can confuse the model if you have examples from the distribution, which is different than your data's one. So for the best results try to match the distribution."
      ],
      "metadata": {
        "id": "dWhLwogce0DJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function Calling\n",
        "\n",
        "We can use tools in OpenAI api as well. Let's see how we can use web search with just the api:"
      ],
      "metadata": {
        "id": "sH-D2q9te0DJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tavily-python -qU"
      ],
      "metadata": {
        "id": "ML_0cLRre0DJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll need a Tavily API key which you can get from [here](https://app.tavily.com/sign-in).\n",
        "\n",
        "Then either use google's secret storage or put it into a file and upload it."
      ],
      "metadata": {
        "id": "g9r9qF5Qe0DJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#os.environ['TAVILITY_API_KEY\"] = open(\".tavily_api_key\").read()\n",
        "os.environ[\"TAVILY_API_KEY\"] = userdata.get(\"tavily_api_key\")\n",
        "\n",
        "from tavily import TavilyClient\n",
        "\n",
        "tavily_client = TavilyClient()\n",
        "\n",
        "response = tavily_client.search(\"Who is Leo Messi?\", topic=\"general\")\n",
        "\n",
        "print(response['results'])"
      ],
      "metadata": {
        "id": "RjdV-AHye0DJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can define a `tool` description for client, so that the model knows how to use it.\n",
        "\n",
        "We will only expose `query` and `topic` parameters.\n",
        "\n",
        "We also need to write short descriptions to explain what the tool and the parameters are for. Note that it's not for you, but for the LLM :) So please make sure you provide a clear explanation.\n",
        "\n",
        "Tool usage is sort of an extension of \"JSON mode\" because in the end we get a dict of parameters, parsed from the JSON."
      ],
      "metadata": {
        "id": "IWspcVJxe0DJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"web-search\",\n",
        "            \"description\": \"Retrieves results from web search\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"query\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"What you search for\",\n",
        "                    },\n",
        "                    \"topic\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"Search topic either 'general' or 'news'\",\n",
        "                        \"enum\": [\"general\", \"news\"]\n",
        "                    },\n",
        "                },\n",
        "                \"required\": [\"query\"],\n",
        "            },\n",
        "        }\n",
        "    },\n",
        "]\n",
        "\n",
        "\n",
        "messages = []\n",
        "messages.append({\"role\": \"system\", \"content\": \"If you are asked about the factual information, create a function call instead. If you already searched, use the results to give an answer.\"})\n",
        "messages.append({\"role\": \"user\", \"content\": \"What is the name of the cat from Shrek?\"})\n",
        "chat_response = nebius_client.chat.completions.create(\n",
        "    messages=messages, tools=tools, model=llama_model\n",
        ")\n",
        "chat_response"
      ],
      "metadata": {
        "id": "Y00ckl3Ie0DJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And we can also try to ask for some news-worthy content to see if LLM decides on a different `topic`."
      ],
      "metadata": {
        "id": "xcU7jtcoe0DK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = []\n",
        "messages.append({\"role\": \"system\", \"content\": \"If you are asked about the factual information, create a function call instead. If you already searched, use the results to give an answer.\"})\n",
        "messages.append({\"role\": \"user\", \"content\": \"What happened in London today?\"})\n",
        "chat_response = nebius_client.chat.completions.create(\n",
        "    messages=messages, tools=tools, model=llama_model\n",
        ")\n",
        "chat_response"
      ],
      "metadata": {
        "id": "gx4QpV3Le0DK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can extract the function usage output from the result"
      ],
      "metadata": {
        "id": "n94smyzfe0DK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_response.choices[0].message.tool_calls[0]"
      ],
      "metadata": {
        "id": "-AMA-dM9e0DK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You might be wondering, why do we include tool usage in structured output topic.\n",
        "\n",
        "Thing is, you can also use this functionality to structure your output. You don't have to use a real function as your tool. Let's use our previous example"
      ],
      "metadata": {
        "id": "hDqCaytPe0DK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"create_rpg_character\",\n",
        "            \"description\": \"Creates a character based on attributes and description\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"name\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"Name of the character\",\n",
        "                    },\n",
        "                    \"age\": {\n",
        "                        \"type\": \"integer\",\n",
        "                        \"description\": \"Age of the character\",\n",
        "                    },\n",
        "                    \"special_skills\": {\n",
        "                        \"type\": \"array\",\n",
        "                        \"description\": \"List of special skills of the character\",\n",
        "                        \"items\": {\n",
        "                            \"type\": \"string\"\n",
        "                        }\n",
        "                    },\n",
        "                    \"traits\": {\n",
        "                        \"type\": \"array\",\n",
        "                        \"description\": \"List of traits of the character\",\n",
        "                        \"items\": {\n",
        "                            \"type\": \"string\"\n",
        "                        }\n",
        "                    },\n",
        "                    \"character_class\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"Class of the character\",\n",
        "                        \"enum\": [\"mage\", \"rogue\", \"barbarian\", \"knight\", \"paladin\"]\n",
        "                    },\n",
        "                    \"origin\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"Origin of the character\",\n",
        "                        \"enum\": [\"human\", \"elf\", \"orc\", \"undead\"]\n",
        "                    },\n",
        "                },\n",
        "                \"required\": [\"name\", \"age\", \"special_skills\", \"traits\", \"character_class\", \"origin\"],\n",
        "            },\n",
        "        }\n",
        "    },\n",
        "]\n"
      ],
      "metadata": {
        "id": "ATabJqL4e0DK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = []\n",
        "messages.append({\"role\": \"system\", \"content\": \"If you are asked to create a character, use `create_rpg_character` tool.\"})\n",
        "messages.append({\"role\": \"user\", \"content\": \"Generate a random character for my new session\"})\n",
        "chat_response = nebius_client.chat.completions.create(\n",
        "    messages=messages, tools=tools, model=llama_model\n",
        ")\n",
        "chat_response.choices[0].message.tool_calls[0].function.arguments"
      ],
      "metadata": {
        "id": "uFf2dqmPe0DK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Practice tasks**\n",
        "\n",
        "If you encounter any difficulties or simply want to see our solutions, feel free to check the [Solutions notebook](https://colab.research.google.com/github/Nebius-Academy/LLM-Engineering-Essentials/blob/main/topic2/2.1_structured_inputs_and_outputs_solutions.ipynb)."
      ],
      "metadata": {
        "id": "4bPMLxcse0DK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1. LLM Information extraction\n",
        "\n",
        "The goal of this task is to create a system, which extracts data about events from free text into a predictable format."
      ],
      "metadata": {
        "id": "7ZlJaY22e0DK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's imagine that you work for a marketing agency, and you need to gather analytics about the passing events dedicated to AI and Machine Learning. For that, you need to process press releases and extract:\n",
        "- Event name,\n",
        "- Event date,\n",
        "- Number of participants,\n",
        "- Number of speakers,\n",
        "- Attendance price.\n",
        "\n",
        "Of course, you can do it manually, but it's much more fun to use Generative AI! So, your task will be to write a function that does this with only one request to OpenAI API.\n",
        "\n",
        "Below there is an example of a press release (generated by ChatGPT, of course, so that both the event and the personae are fictional). All of them are in the press_releases.zip archive in the hometask week 1 folder.\n",
        "\n",
        "<blockquote>\n",
        "<p>PRESS RELEASE\n",
        "\n",
        "InnovAI Summit 2023: A Glimpse into the Future of Artificial Intelligence</p>\n",
        "\n",
        "City of Virtue, Cyberspace - November 8, 2023 - The most anticipated event of the year, InnovAI Summit 2023, successfully concluded last weekend, on November 5, 2023. Held in the state-of-the-art VirtuTech Arena, the summit saw a massive turnout of over 3,500 participants, from brilliant AI enthusiasts and researchers to pioneers in the field.\n",
        "\n",
        "Esteemed speakers took to the stage to shed light on the latest breakthroughs, practical implementations, and ethical considerations in AI. Dr. Evelyn Quantum, renowned for her groundbreaking work on Quantum Machine Learning, emphasized the importance of this merger and how it's revolutionizing computing as we know it. Another keynote came from Prof. Leo Nexus, whose current project 'AI for Sustainability' highlights the symbiotic relationship between nature and machine, aiming to use AI in restoring our planet's ecosystems.\n",
        "\n",
        "This year's panel discussion, moderated by the talented Dr. Ada Neura, featured lively debates on the limits of AI in creative arts. Renowned digital artist, Felix Vortex, showcased how he uses generative adversarial networks to create surreal art pieces, while bestselling author, Iris Loom, explained her experiments with AI-assisted story crafting.\n",
        "\n",
        "Among other highlights were hands-on workshops, interactive Q&A sessions, and an 'AI & Ethics' debate which was particularly well-received, emphasizing the need for transparency and fairness in AI models. An exclusive 'Start-up Alley' allowed budding entrepreneurs to showcase their innovations, gaining attention from global venture capitalists and media.\n",
        "\n",
        "The event wrapped up with an announcement for InnovAI Summit 2024, set to be even grander. Participants left with a renewed enthusiasm for the vast possibilities that the AI and ML world promises.\n",
        "\n",
        "For media inquiries, please contact:\n",
        "Jane Cipher\n",
        "Director of Communications, InnovAI Summit\n",
        "Email: jane.cipher@innovai.org\n",
        "Phone: +123-4567-8910</p>\n",
        "</blockquote>\n",
        "\n",
        "More specifically, you should write a function\n",
        "\n",
        "```python\n",
        "parse_press_release(pr: str) -> dict\n",
        "```\n",
        "\n",
        "where the output should be in the format\n",
        "\n",
        "```python\n",
        "{\n",
        "  name: 'InnovAI Summit 2023',\n",
        "  date: '08.11.2023',\n",
        "  n_participants: 3500,\n",
        "  n_speakers: 4,\n",
        "  price:\n",
        "}\n",
        "```\n",
        "\n",
        "If any of the four characteristics is not mentioned in the text, put `None` in the respective field.\n",
        "\n",
        "At the end, calculate the statistics of right answers and analyse what kind of mistakes you \"model\" makes the most."
      ],
      "metadata": {
        "id": "cEvtiZy6e0DK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hints and suggestions:**\n",
        "- It's gonna be more convenient to experiment in Nebius AI Studio's playground https://studio.nebius.com/playground.\n",
        "- You need to be very accurate with what you want from the model.\n",
        "- It will help if you specify in the prompt that the output should be in JSON format, this way you will spend less time parsing the output. But be careful. Though some models are easily prompted to output a JSON, please check the output format. It may contain excessive formatting, for example:\n",
        "<pre><code>```json\n",
        "{\"name\": \"InnovAI Summit 2023\", ...}\n",
        "```</pre></code>\n",
        "Actually, examining LLM outputs and their format is a must when working with them\n",
        "\n",
        "- Please be careful with the details. For example, Jane Cipher in the text above is not a speaker and shouldn't be counter as such (how to get rid of a contact person?). Also pay attention to the date format,\n",
        "- If the model is too wilful with the output format, don't hesitate to show some examples. Decreasing the temperature of predictions can help reduce the creativity of the answer, which is what we want for such task.\n",
        "- Debugging an LLM-powered application may become a tough business. When you think that you've polished it, an LLM can still surprise you. So, we don't expect 100% accuracy in this task, but we expect that you do your best to achieve high quality results."
      ],
      "metadata": {
        "id": "eySv4YHWe0DK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bonus points**:\n",
        "Try writing the solution using:\n",
        "- Structured JSON Output\n",
        "- Guiding JSON Output using Structures"
      ],
      "metadata": {
        "id": "vbJYFRc6e0DK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "press_release = \"\"\"PRESS RELEASE\n",
        "\n",
        "InnovAI Summit 2023: A Glimpse into the Future of Artificial Intelligence\n",
        "\n",
        "City of Virtue, Cyberspace - November 8, 2023 - The most anticipated event of the year, InnovAI Summit 2023, successfully concluded last weekend, on November 5, 2023. Held in the state-of-the-art VirtuTech Arena, the summit saw a massive turnout of over 3,500 participants, from brilliant AI enthusiasts and researchers to pioneers in the field.\n",
        "\n",
        "Esteemed speakers took to the stage to shed light on the latest breakthroughs, practical implementations, and ethical considerations in AI. Dr. Evelyn Quantum, renowned for her groundbreaking work on Quantum Machine Learning, emphasized the importance of this merger and how it's revolutionizing computing as we know it. Another keynote came from Prof. Leo Nexus, whose current project 'AI for Sustainability' highlights the symbiotic relationship between nature and machine, aiming to use AI in restoring our planet's ecosystems.\n",
        "\n",
        "This year's panel discussion, moderated by the talented Dr. Ada Neura, featured lively debates on the limits of AI in creative arts. Renowned digital artist, Felix Vortex, showcased how he uses generative adversarial networks to create surreal art pieces, while bestselling author, Iris Loom, explained her experiments with AI-assisted story crafting.\n",
        "\n",
        "Among other highlights were hands-on workshops, interactive Q&A sessions, and an 'AI & Ethics' debate which was particularly well-received, emphasizing the need for transparency and fairness in AI models. An exclusive 'Start-up Alley' allowed budding entrepreneurs to showcase their innovations, gaining attention from global venture capitalists and media.\n",
        "\n",
        "The event wrapped up with an announcement for InnovAI Summit 2024, set to be even grander. Participants left with a renewed enthusiasm for the vast possibilities that the AI and ML world promises.\n",
        "\n",
        "For media inquiries, please contact: Jane Cipher Director of Communications, InnovAI Summit Email: jane.cipher@innovai.org Phone: +123-4567-8910\"\"\""
      ],
      "metadata": {
        "id": "zJspUelOe0DL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_press_release(pr: str) -> dict:\n",
        "    pass"
      ],
      "metadata": {
        "id": "fXP5oO41e0DL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing\n",
        "\n",
        "We've prepared a small dataset for you to test your prompt on. Provided you've written your function, try running the following code. At the end you also have an opportunity to look at the results in a table side-by-side in with_results.csv. Your goal is to get at least 60% of fields right.."
      ],
      "metadata": {
        "id": "rXJF_b3de0DL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade gdown\n",
        "!gdown -O press_release_extraction.csv https://docs.google.com/spreadsheets/d/15IGdc3MV8864lxrLxsug0Ij480p76T1EAwBM7WGT_OI/export?format=csv"
      ],
      "metadata": {
        "id": "dTVTe284e0DL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas\n",
        "pr_df = pandas.read_csv(\"press_release_extraction.csv\")\n",
        "pr_df.head()"
      ],
      "metadata": {
        "id": "OJjcT2pSe0DL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pr_df.pr_parsed[0]"
      ],
      "metadata": {
        "id": "sDGJ1vKxe0DL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "parsed_list = []\n",
        "fields = {\n",
        "    \"name\": str,\n",
        "    \"date\": str,\n",
        "    \"n_speakers\": int,\n",
        "    \"n_participants\": int,\n",
        "    \"price\": str\n",
        "}\n",
        "correct_fields = 0\n",
        "for row in pr_df.itertuples():\n",
        "    parsed_release = parse_press_release(row.pr_text)\n",
        "    parsed_list.append(json.dumps(parsed_release, indent=4))\n",
        "    golden = json.loads(row.pr_parsed)\n",
        "    for field, field_type in fields.items():\n",
        "        golden_field = golden[field]\n",
        "        parsed_field = parsed_release.get(field)\n",
        "        try:\n",
        "            parsed_field = field_type(parsed_field)\n",
        "        except (ValueError, TypeError):\n",
        "            pass\n",
        "        if golden_field == parsed_field:\n",
        "            correct_fields += 1\n",
        "        else:\n",
        "            print(f\"For {golden['name']} {field} {parsed_release.get(field)} doesn't seem the same as {golden[field]}\")\n",
        "\n",
        "print(f\"Correctly extracted {correct_fields} out of {5*len(pr_df)}\")"
      ],
      "metadata": {
        "id": "RSvVEFCWe0DL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bonus points\n",
        "- Try and compare different ways of establishing the correct answer formatting\n",
        "- Try and compare different LLMs"
      ],
      "metadata": {
        "id": "lcGzo04oe0DL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2. Character localization\n",
        "\n",
        "Cool thing about structured output, is that it's very easy to make a translated version of a specific dataset, taking into account all the context and outputing in a format, which is super easy to parse. Let's try this on MMLU.\n",
        "\n",
        "**Task:** Write a function which inputs a sample from MMLU and outputs a translated version, using structured outputs.\n",
        "\n",
        "Tip: make sure that the correct answer didn't change."
      ],
      "metadata": {
        "id": "EDQRdBcLe0DL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU datasets"
      ],
      "metadata": {
        "id": "wsZ7lgLXe0DM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from pydantic import BaseModel\n",
        "\n",
        "class MMLUSample(BaseModel):\n",
        "    ...\n",
        "\n",
        "def translate_mmlu_sample(sample: MMLUSample, target_language: str) -> MMLUSample:\n",
        "    ..."
      ],
      "metadata": {
        "id": "-iMfgJG7e0DM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from pydantic import BaseModel\n",
        "\n",
        "class MMLUSample(BaseModel):\n",
        "    question: str\n",
        "    A: str\n",
        "    B: str\n",
        "    C: str\n",
        "    D: str\n",
        "    correct_answer: str\n",
        "\n",
        "def translate_mmlu_sample(sample: MMLUSample, target_language: str) -> MMLUSample:\n",
        "    completion = nebius_client.chat.completions.create(\n",
        "        model=llama_model,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"Translate this MMLU sample into {target_language}\" \\\n",
        "                f\"Question: {sample.question}\\n\" \\\n",
        "                f\"A: {sample.A}\\n\" \\\n",
        "                f\"B: {sample.B}\\n\" \\\n",
        "                f\"C: {sample.C}\\n\" \\\n",
        "                f\"D: {sample.D}\\n\" \\\n",
        "                f\"Correct answer: {sample.correct_answer}\\n\" \\\n",
        "                f\"Translated sample:\"\n",
        "            }\n",
        "        ],\n",
        "        extra_body={\n",
        "            \"guided_json\": MMLUSample.model_json_schema()\n",
        "        },\n",
        "    )\n",
        "\n",
        "    translated = MMLUSample.model_validate_json(completion.choices[0].message.content)\n",
        "    if translated.correct_answer != sample.correct_answer:\n",
        "        translated.correct_answer = sample.correct_answer\n",
        "    return translated"
      ],
      "metadata": {
        "id": "eieWNf6Ze0DM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mmlu_sample = MMLUSample(\n",
        "    question = \"Which of the following statements about Ethernets is typically FALSE?\",\n",
        "    A = \"Ethernets use circuit switching to send messages.\",\n",
        "    B = \"Ethernets use buses with multiple masters.\",\n",
        "    C = \"Ethernet protocols use a collision-detection method to ensure that messages are transmitted properly.\",\n",
        "    D = \"Networks connected by Ethernets are limited in length to a few hundred meters.\",\n",
        "    correct_answer = \"A\"\n",
        ")\n",
        "\n",
        "translate_mmlu_sample(mmlu_sample, target_language=\"German\")"
      ],
      "metadata": {
        "id": "6EqyzfJxe0DM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's remember the code we've written for MMLU evaluator and add a little twist:\n",
        "\n",
        "We'll have both topic and language in which we want to evaluate the model."
      ],
      "metadata": {
        "id": "uwoVgXi7e0DM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets -q"
      ],
      "metadata": {
        "id": "KIB_2RFYe0DM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task**: Modify the following MMLUEvaluator code so that it can also translate the input question and evaluate the performance in a different language."
      ],
      "metadata": {
        "id": "TIqzv_AJe0DM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from typing import List, Dict, Tuple\n",
        "import json\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "class MMLUEvaluator:\n",
        "    def __init__(self, system_prompt: str = None, prompt: str = None,\n",
        "                 topic: str = \"high_school_mathematics\"):\n",
        "        \"\"\"\n",
        "        Initialize the MMLU evaluator.\n",
        "\n",
        "        Args:\n",
        "            system_prompt: Optional system prompt for the model\n",
        "            prompt: Custom prompt for the model\n",
        "            topic: Which topic to choose\n",
        "        \"\"\"\n",
        "\n",
        "        self.topic = topic\n",
        "        self.topic_prettified = topic.replace(\"_\", \" \")\n",
        "        self.system_prompt = system_prompt or f\"You are an expert in {self.topic_prettified}.\"\n",
        "\n",
        "        self.prompt = \"\"\"You are given a question in {topic_prettified} with four answer options labeled by A, B, C, and D.\n",
        "You need to ponder the question and justify the choice of one of the options A, B, C, or D.\n",
        "At the end, do write the chosen answer option A, B, C, D after #ANSWER:\n",
        "Now, take a deep breath and work out this problem step by step. If you do well, I'll tip you 200$.\n",
        "\n",
        "QUESTION: {question}\n",
        "\n",
        "ANSWER OPTIONS:\n",
        "A: {A}\n",
        "B: {B}\n",
        "C: {C}\n",
        "D: {D}\n",
        "\"\"\"\n",
        "\n",
        "        self.questions, self.choices, self.answers = self.load_mmlu_data(topic=self.topic)\n",
        "\n",
        "    def load_mmlu_data(self, topic: str) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Load MMLU test data on a given topic.\n",
        "\n",
        "        Args:\n",
        "            topic: Which topic to choose\n",
        "\n",
        "        Returns:\n",
        "            DataFrame with questions and answers\n",
        "        \"\"\"\n",
        "\n",
        "        dataset = load_dataset(\"cais/mmlu\", topic, split=\"test\")\n",
        "\n",
        "        dataset = dataset\n",
        "        dataset = pd.DataFrame(dataset)\n",
        "\n",
        "        # Load questions and choices separately\n",
        "        questions = dataset[\"question\"]\n",
        "        choices = pd.DataFrame(\n",
        "            data=dataset[\"choices\"].tolist(), columns=[\"A\", \"B\", \"C\", \"D\"]\n",
        "        )\n",
        "        # In the dataset, true answer labels are in 0-3 format;\n",
        "        # We convert it to A-D\n",
        "        answers = dataset[\"answer\"].map(lambda ans: {0: \"A\", 1: \"B\", 2: \"C\", 3: \"D\"}[ans])\n",
        "\n",
        "        return questions, choices, answers\n",
        "\n",
        "    def extract_answer(self, solution: str) -> str:\n",
        "        \"\"\"\n",
        "        Extract the letter answer from model's response.\n",
        "\n",
        "        Args:\n",
        "            response: Raw model response\n",
        "\n",
        "        Returns:\n",
        "            Extracted answer letter (A, B, C, D, or Failed to parse)\n",
        "        \"\"\"\n",
        "        # Look for a single letter answer in the response\n",
        "        try:\n",
        "            answer = solution.split('#ANSWER:')[1].strip()\n",
        "        except:\n",
        "            answer = \"Failed to parse\"\n",
        "        return answer\n",
        "\n",
        "    def evaluate_single_question(self, question: str, choices: Dict[str, str],\n",
        "                                 correct_answer: str,\n",
        "                                 client, model) -> Tuple[bool, str]:\n",
        "        \"\"\"\n",
        "        Evaluate a single question.\n",
        "\n",
        "        Args:\n",
        "            question: Formatted question string\n",
        "            correct_answer: Correct answer letter\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (is_correct, extracted_answer, model_response)\n",
        "        \"\"\"\n",
        "        try:\n",
        "            model_response = answer_with_llm(\n",
        "                prompt=self.prompt.format(\n",
        "                    client=client, model=model,\n",
        "                    topic_prettified=self.topic_prettified,\n",
        "                    question=question,\n",
        "                    A=choices['A'], B=choices['B'], C=choices['C'], D=choices['D']\n",
        "                ),\n",
        "                system_prompt=self.system_prompt,\n",
        "                prettify=False\n",
        "            )\n",
        "            answer = self.extract_answer(model_response)\n",
        "            is_correct = (answer.upper() == correct_answer.upper())\n",
        "            return is_correct, answer, model_response\n",
        "        except Exception as e:\n",
        "            print(f\"Error evaluating question: {e}\")\n",
        "            return False, None, None\n",
        "\n",
        "    def run_evaluation(self, client=nebius_client, model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
        "                       n_questions=50) -> Dict:\n",
        "        \"\"\"\n",
        "        Run evaluation of a given model on the first n_questions.\n",
        "\n",
        "        Args:\n",
        "            client: Which client to use (OpenAI or Nebius)\n",
        "            model: Which model to use\n",
        "            n_questions: How many first questions to take\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with evaluation metrics\n",
        "        \"\"\"\n",
        "        evaluation_log = []\n",
        "        correct_count = 0\n",
        "\n",
        "        if n_questions:\n",
        "            n_questions = min(n_questions, len(self.questions))\n",
        "        else:\n",
        "            n_questions = len(self.questions)\n",
        "\n",
        "        for i in tqdm(range(n_questions)):\n",
        "            is_correct, answer, model_response = self.evaluate_single_question(\n",
        "                question=self.questions[i],\n",
        "                choices=self.choices.iloc[i],\n",
        "                correct_answer=self.answers[i],\n",
        "                client=client,\n",
        "                model=model,\n",
        "            )\n",
        "\n",
        "            if is_correct:\n",
        "                correct_count += 1\n",
        "\n",
        "            evaluation_log.append({\n",
        "                'answer': answer,\n",
        "                'model_response': model_response,\n",
        "                'is_correct': is_correct\n",
        "            })\n",
        "\n",
        "        accuracy = correct_count / n_questions\n",
        "        evaluation_results = {\n",
        "            'accuracy': accuracy,\n",
        "            'evaluation_log': evaluation_log\n",
        "        }\n",
        "\n",
        "        return evaluation_results\n"
      ],
      "metadata": {
        "id": "KmbtCJnNe0DM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing"
      ],
      "metadata": {
        "id": "78Eqh6jhe0DM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator = MMLUEvaluator(topic=\"medical_genetics\", language=\"English\")\n",
        "\n",
        "results = evaluator.run_evaluation(model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
        "                         n_questions=50)\n",
        "print(f'\\nAccuracy: {results[\"accuracy\"]}')"
      ],
      "metadata": {
        "id": "DpQ9Lp6_e0DM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator_de = MMLUEvaluator(topic=\"medical_genetics\", language=\"German\")\n",
        "\n",
        "results_de = evaluator_de.run_evaluation(model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
        "                         n_questions=10)\n",
        "print(f'\\nAccuracy: {results_de[\"accuracy\"]}')"
      ],
      "metadata": {
        "id": "-_jrw-z0e0DN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FIW2nMg8e0DN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}