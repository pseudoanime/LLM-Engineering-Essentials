{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMh81SzBFMCR8TE+fg6yfc5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexUmnov/LLM-Engineering-Essentials/blob/main/topic2/2.2_llm_workflows_solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Engineering Essentials by Nebius Academy\n",
        "\n",
        "Course github: [link](https://github.com/Nebius-Academy/LLM-Engineering-Essentials/tree/main)\n",
        "\n",
        "# 2.2. Structured Inputs and Output"
      ],
      "metadata": {
        "id": "K0jTngEqySxI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Practice task solutions**"
      ],
      "metadata": {
        "id": "OcEmsa1vyWTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai -qU"
      ],
      "metadata": {
        "id": "m3UBYmNgE90I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f4c2c2d-e5be-476e-a43a-cda2c2ce5e3e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/683.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m675.8/683.3 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m683.3/683.3 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "import os\n",
        "\n",
        "os.environ['NEBIUS_API_KEY'] = userdata.get(\"nebius_api_key\")\n",
        "\n",
        "nebius_client = OpenAI(\n",
        "    base_url=\"https://api.studio.nebius.ai/v1/\",\n",
        "    api_key=os.environ.get(\"NEBIUS_API_KEY\"),\n",
        ")\n",
        "\n",
        "llama_model = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
        "\n",
        "def prettify_string(text, max_line_length=80):\n",
        "    \"\"\"Prints a string with line breaks at spaces to prevent horizontal scrolling.\n",
        "\n",
        "    Args:\n",
        "        text: The string to print.\n",
        "        max_line_length: The maximum length of each line.\n",
        "    \"\"\"\n",
        "\n",
        "    output_lines = []\n",
        "    lines = text.split(\"\\n\")\n",
        "    for line in lines:\n",
        "        current_line = \"\"\n",
        "        words = line.split()\n",
        "        for word in words:\n",
        "            if len(current_line) + len(word) + 1 <= max_line_length:\n",
        "                current_line += word + \" \"\n",
        "            else:\n",
        "                output_lines.append(current_line.strip())\n",
        "                current_line = word + \" \"\n",
        "        output_lines.append(current_line.strip())  # Append the last line\n",
        "    return \"\\n\".join(output_lines)\n",
        "\n",
        "def answer_with_llm(prompt: str,\n",
        "                    system_prompt=\"You are a helpful assistant\",\n",
        "                    max_tokens=512,\n",
        "                    client=nebius_client,\n",
        "                    model=llama_model,\n",
        "                    prettify=True,\n",
        "                    temperature=None) -> str:\n",
        "\n",
        "    messages = []\n",
        "\n",
        "    if system_prompt:\n",
        "        messages.append(\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": system_prompt\n",
        "            }\n",
        "        )\n",
        "\n",
        "    messages.append(\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt\n",
        "        }\n",
        "    )\n",
        "\n",
        "    completion = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        max_tokens=max_tokens,\n",
        "        temperature=temperature\n",
        "    )\n",
        "\n",
        "    if prettify:\n",
        "        return prettify_string(completion.choices[0].message.content)\n",
        "    else:\n",
        "        return completion.choices[0].message.content\n"
      ],
      "metadata": {
        "id": "GeFeBF0mKYVL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Task 1. Character localization\n",
        "\n",
        "Let's add localization to our simple chat NPC class from Topic 1.\n",
        "\n",
        "Your task will be to implement the following localized chat pipeline:\n",
        "- The user's input is translated into English,\n",
        "- The NPC answers an English query in English (already implemented)\n",
        "- The NPC's answer is translated into the target language, and the translation is returned to the user."
      ],
      "metadata": {
        "id": "fNgipjNwzFhu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Class copypaste\n",
        "import os\n",
        "\n",
        "#with open(\"nebius_api_key\", \"r\") as file:\n",
        "#    nebius_api_key = file.read().strip()\n",
        "\n",
        "#os.environ[\"NEBIUS_API_KEY\"] = nebius_api_key\n",
        "from google.colab import userdata\n",
        "os.environ[\"NEBIUS_API_KEY\"] = userdata.get(\"nebius_api_key\")\n",
        "\n",
        "from collections import defaultdict, deque\n",
        "from openai import OpenAI\n",
        "from typing import Dict, Any, Optional\n",
        "import datetime\n",
        "import string\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class NPCConfig:\n",
        "    world_description: str\n",
        "    character_description: str\n",
        "    history_size: int = 10\n",
        "    has_scratchpad: bool = False\n",
        "\n",
        "class NPCFactoryError(Exception):\n",
        "    \"\"\"Base exception class for NPC Factory errors.\"\"\"\n",
        "    pass\n",
        "\n",
        "class NPCNotFoundError(NPCFactoryError):\n",
        "    \"\"\"Raised when trying to interact with a non-existent NPC.\"\"\"\n",
        "    def __init__(self, npc_id: str):\n",
        "        self.npc_id = npc_id\n",
        "        super().__init__(f\"NPC with ID '{npc_id}' not found\")\n",
        "\n",
        "class SimpleChatNPC:\n",
        "    def __init__(self, client: OpenAI, model: str, config: NPCConfig):\n",
        "        self.client = client\n",
        "        self.model = model\n",
        "        self.config = config\n",
        "        self.chat_histories = defaultdict(lambda: deque(maxlen=config.history_size))\n",
        "\n",
        "    def localize_input(self, input_text: str) -> str:\n",
        "        \"\"\"Translate user input into English.\"\"\"\n",
        "        return self.client.chat.completions.create(\n",
        "            messages=[\n",
        "                {\n",
        "                    'role': 'user',\n",
        "                    'content': f'Translate the following text into English:\\n{input_text}. Only output the translation itself.'\n",
        "                }\n",
        "            ],\n",
        "            model=self.model,\n",
        "        ).choices[0].message.content\n",
        "\n",
        "    def localize_output(self, output_text: str, target_language: str) -> str:\n",
        "        \"\"\"Translate the output into the target language.\"\"\"\n",
        "        return self.client.chat.completions.create(\n",
        "            messages=[\n",
        "                {\n",
        "                    'role': 'user',\n",
        "                    'content': f'Translate the following text into {target_language}:\\n{output_text}. Only output the translation itself.'\n",
        "                }\n",
        "            ],\n",
        "            model=self.model,\n",
        "        ).choices[0].message.content\n",
        "\n",
        "    def get_system_message(self) -> Dict[str, str]:\n",
        "        \"\"\"Returns the system message that defines the NPC's behavior.\"\"\"\n",
        "        character_description = self.config.character_description\n",
        "\n",
        "        if self.config.has_scratchpad:\n",
        "            character_description += \"\"\"\n",
        "You can use scratchpad for thinking before you answer: whatever you output between #SCRATCHPAD and #ANSWER won't be shown to anyone.\n",
        "You start your output with #SCRATCHPAD and after you've done thinking, you #ANSWER\"\"\"\n",
        "\n",
        "        return {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": f\"\"\"WORLD SETTING: {self.config.world_description}\n",
        "###\n",
        "{character_description}\"\"\"\n",
        "        }\n",
        "\n",
        "    def chat(self, user_message: str, user_id: str, user_language: str) -> str:\n",
        "        \"\"\"Process a user message and return the NPC's response.\"\"\"\n",
        "        messages = [self.get_system_message()]\n",
        "\n",
        "        # Add conversation history\n",
        "        history = list(self.chat_histories[user_id])\n",
        "        if history:\n",
        "            messages.extend(history)\n",
        "\n",
        "        # Add new user message\n",
        "        if user_language != \"English\":\n",
        "            user_message_dict = {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": self.localize_input(user_message)\n",
        "            }\n",
        "        else:\n",
        "            user_message_dict = {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": user_message\n",
        "            }\n",
        "        self.chat_histories[user_id].append(user_message_dict)\n",
        "        messages.append(user_message_dict)\n",
        "\n",
        "        try:\n",
        "            completion = self.client.chat.completions.create(\n",
        "                model=self.model,\n",
        "                messages=messages,\n",
        "                temperature=0.6\n",
        "            )\n",
        "\n",
        "            response = completion.choices[0].message.content\n",
        "\n",
        "            # Handle scratchpad if enabled\n",
        "            response_clean = response\n",
        "            if self.config.has_scratchpad:\n",
        "                import re\n",
        "                scratchpad_match = re.search(r\"#SCRATCHPAD(:?)(.*?)#ANSWER(:?)\", response, re.DOTALL)\n",
        "                if scratchpad_match:\n",
        "                    response_clean = response[scratchpad_match.end():].strip()\n",
        "\n",
        "            if user_language != \"English\":\n",
        "                response_clean = self.localize_output(response_clean, user_language)\n",
        "\n",
        "            # Store response in history, including the scratchpad\n",
        "            self.chat_histories[user_id].append({\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": response\n",
        "            })\n",
        "\n",
        "            # Return the message to the user without a scratchpad\n",
        "            return response_clean\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"Error: {str(e)}\"\n",
        "\n",
        "class NPCFactory:\n",
        "    def __init__(self, client: OpenAI, model: str):\n",
        "        self.client = client\n",
        "        self.model = model\n",
        "        self.npcs: Dict[str, SimpleChatNPC] = {}\n",
        "        self.user_ids: Dict[str, str] = {}  # username -> user_id mapping\n",
        "        self.user_preffered_languages: Dict[str, str] = {}  # user_id -> language mapping\n",
        "\n",
        "    def set_user_language(self, user_id: str, language: str):\n",
        "        \"\"\"Set the preferred language for a user.\"\"\"\n",
        "        self.user_preffered_languages[user_id] = language\n",
        "\n",
        "    def generate_id(self) -> str:\n",
        "        \"\"\"Generate a random unique identifier.\"\"\"\n",
        "        return ''.join(random.choice(string.ascii_letters) for _ in range(8))\n",
        "\n",
        "    def register_user(self, username: str) -> str:\n",
        "        \"\"\"Register a new user and return their unique ID.\n",
        "        If username already exists, appends a numerical suffix.\"\"\"\n",
        "        base_username = username\n",
        "        suffix = 1\n",
        "\n",
        "        # Keep trying with incremented suffixes until we find an unused name\n",
        "        while username in self.user_ids:\n",
        "            username = f\"{base_username}_{suffix}\"\n",
        "            suffix += 1\n",
        "\n",
        "        user_id = self.generate_id()\n",
        "        self.user_ids[username] = user_id\n",
        "        return user_id\n",
        "\n",
        "    def register_npc(self, world_description: str, character_description: str,\n",
        "                     history_size: int = 10, has_scratchpad: bool = False) -> str:\n",
        "        \"\"\"Create and register a new NPC, returning its unique ID.\"\"\"\n",
        "        npc_id = self.generate_id()\n",
        "\n",
        "        config = NPCConfig(\n",
        "            world_description=world_description,\n",
        "            character_description=character_description,\n",
        "            history_size=history_size,\n",
        "            has_scratchpad=has_scratchpad\n",
        "        )\n",
        "\n",
        "        self.npcs[npc_id] = SimpleChatNPC(self.client, self.model, config)\n",
        "        return npc_id\n",
        "\n",
        "    def chat_with_npc(self, npc_id: str, user_id: str, message: str) -> str:\n",
        "        \"\"\"Send a message to a specific NPC from a specific user.\n",
        "\n",
        "        Args:\n",
        "            npc_id: The unique identifier of the NPC\n",
        "            user_id: The unique identifier of the user\n",
        "            message: The message to send\n",
        "\n",
        "        Returns:\n",
        "            The NPC's response\n",
        "\n",
        "        Raises:\n",
        "            NPCNotFoundError: If the specified NPC doesn't exist\n",
        "        \"\"\"\n",
        "        user_language = self.user_preffered_languages.get(user_id, \"English\")\n",
        "        if npc_id not in self.npcs:\n",
        "            raise NPCNotFoundError(npc_id)\n",
        "\n",
        "        npc = self.npcs[npc_id]\n",
        "        return npc.chat(message, user_id, user_language=self.user_preffered_languages[user_id])\n",
        "\n",
        "    def get_npc_chat_history(self, npc_id: str, user_id: str) -> list:\n",
        "        \"\"\"Retrieve chat history between a specific user and NPC.\n",
        "\n",
        "        Args:\n",
        "            npc_id: The unique identifier of the NPC\n",
        "            user_id: The unique identifier of the user\n",
        "\n",
        "        Returns:\n",
        "            List of message dictionaries containing the chat history\n",
        "\n",
        "        Raises:\n",
        "            NPCNotFoundError: If the specified NPC doesn't exist\n",
        "        \"\"\"\n",
        "        if npc_id not in self.npcs:\n",
        "            raise NPCNotFoundError(npc_id)\n",
        "\n",
        "        return list(self.npcs[npc_id].chat_histories[user_id])"
      ],
      "metadata": {
        "id": "tdMbiABGzhA8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "# Nebius uses the same OpenAI() class, but with additional details\n",
        "client = OpenAI(\n",
        "    base_url=\"https://api.studio.nebius.ai/v1/\",\n",
        "    api_key=os.environ.get(\"NEBIUS_API_KEY\"),\n",
        ")\n",
        "\n",
        "model = \"meta-llama/Meta-Llama-3.1-405B-Instruct\"\n",
        "\n",
        "# Creating a factory\n",
        "npc_factory = NPCFactory(client=client, model=model)"
      ],
      "metadata": {
        "id": "C9za0tPtaGoQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Register a user\n",
        "user_id = npc_factory.register_user(\"Alice\")\n",
        "\n",
        "# Set user preffered language\n",
        "preffered_language = \"Old English\"\n",
        "npc_factory.set_user_language(user_id, preffered_language)\n",
        "\n",
        "# Create an NPC\n",
        "npc_id = npc_factory.register_npc(\n",
        "    world_description=\"Medieval London, XIII century\",\n",
        "    character_description=\"A knight at Edward I's court\",\n",
        "    has_scratchpad=False\n",
        ")"
      ],
      "metadata": {
        "id": "NjNokneB2P4k"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prettify_string(text, max_line_length=80):\n",
        "    \"\"\"Prints a string with line breaks at spaces to prevent horizontal scrolling.\n",
        "\n",
        "    Args:\n",
        "        text: The string to print.\n",
        "        max_line_length: The maximum length of each line.\n",
        "    \"\"\"\n",
        "\n",
        "    output_lines = []\n",
        "    lines = text.split(\"\\n\")\n",
        "    for line in lines:\n",
        "        current_line = \"\"\n",
        "        words = line.split()\n",
        "        for word in words:\n",
        "            if len(current_line) + len(word) + 1 <= max_line_length:\n",
        "                current_line += word + \" \"\n",
        "            else:\n",
        "                output_lines.append(current_line.strip())\n",
        "                current_line = word + \" \"\n",
        "        output_lines.append(current_line.strip())  # Append the last line\n",
        "    return \"\\n\".join(output_lines)"
      ],
      "metadata": {
        "id": "DpWeu8623GRB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can hack our own code a bit and use translation features to test the system.\n",
        "\n",
        "npc = npc_factory.npcs[npc_id]\n",
        "\n",
        "message = \"Hello, who are you and what brings you here?\"\n",
        "message_translated = npc.localize_output(message, preffered_language)\n",
        "print(f\"Translated message: {message_translated}\")\n",
        "\n",
        "response = npc_factory.chat_with_npc(\n",
        "    npc_id,\n",
        "    user_id,\n",
        "    message_translated\n",
        ")\n",
        "print(\"Original answer\")\n",
        "print(prettify_string(response))\n",
        "print(\"Translated answer\")\n",
        "print(prettify_string(npc.localize_input(response)))"
      ],
      "metadata": {
        "id": "vCcOYeRP3MG2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ccdbb70-f56e-41ae-b357-c30310baac75"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated message: \"Hailo, hwā eart þū and hwæt bringeþ þē hēr?\"\n",
            "Original answer\n",
            "\"Godes bletsunge si þe Kyng, gode syr. Ic eom Syr Eadweard de Montfort, eadmod\n",
            "cniht on þeowan to ure miclan cyning, Eadweard I. Ic eom cuman to attendantenne\n",
            "þe cyninges heall, swa swa is min pliht, and to offre mine ræd on þingum rycst\n",
            "and gewinne, gyf hit si bidden of me. Hit is wyrðo to þeowan þe Kyng, and ic\n",
            "eom stol on stodan betwixo þa æþelan cnihtas þe ure ryc weriað. Ic bidde, secga\n",
            "me, hwæt þu sy, þæt ic witan mote hwem ic spece.\".\n",
            "Translated answer\n",
            "\"God's blessings be upon the King, good sir. I am Sir Edward de Montfort, a\n",
            "humble knight at the service of our great king, Edward I. I have come to attend\n",
            "the king's hall, as is my duty, and to offer my counsel on matters of power and\n",
            "war, if it be asked of me. It is an honor to serve the King, and I am proud to\n",
            "stand among the noble knights who defend our realm. I pray you, tell me, who\n",
            "are you, that I may know to whom I speak.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2. Translating poetry\n",
        "\n",
        "LLMs are notoriously bad at translating poetry. The resulting poems rarely have good rhyme and rhytm. Let's try to naively translate some Humpty Dumpty rhyme to a language of your choice:"
      ],
      "metadata": {
        "id": "eEGu6PFqjlrk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "language = \"Dutch\"\n",
        "poem = \"\"\"Humpty Dumpty sat on a wall,\n",
        "Humpty Dumpty had a great fall.\n",
        "All the king's horses and all the king's men\n",
        "Couldn't put Humpty together again.\"\"\"\n",
        "\n",
        "print(answer_with_llm(\n",
        "    f\"Translate the following children's rhyme to {language}\\n{poem}\"\n",
        "))"
      ],
      "metadata": {
        "id": "od4336JzjxYv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7c82e25-825d-42d0-d58d-1c4d4e58fd82"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's the translation of the rhyme to Dutch:\n",
            "\n",
            "Humpty Dumpty zat op een muur,\n",
            "Humpty Dumpty had een grote val.\n",
            "Alle paarden van de koning en alle mannen van de koning\n",
            "Konden Humpty niet meer in elkaar zetten.\n",
            "\n",
            "Note: In some Dutch-speaking countries, the name \"Humpty Dumpty\" is translated\n",
            "to \"Hompie Dompie\" or \"Hompie Dompie\", but I've kept the original name in the\n",
            "translation to maintain consistency.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is hardly a good translation because the rhyme is completely broken.\n",
        "\n",
        "You task is to create a chain of LLM calls to do the following steps in translating a poem:\n",
        "\n",
        "1. Do a naive literal translation to preserve the meaning\n",
        "2. Rewrite the translation to retain rhyme and rhythm but perhaps loosing a bit of meaning.\n",
        "3. Finally have an editor look at both original and translation and make final touch ups.\n",
        "\n",
        "We encourage you to try and prompt your LLMs to do the job of \"Translator\", \"Editor\" and so on.\n",
        "Also choose the language you can understand best so that you can evaluate the result well (you can also change the original to some other language)"
      ],
      "metadata": {
        "id": "Cp832NFplDKN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_in_stages(input: str, language: str) -> str:\n",
        "    pass"
      ],
      "metadata": {
        "id": "jJvC7DthlG_x"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "literal_translator_system_prompt = f\"\"\"\n",
        "You are a translator, whose task is to translate whatever you receive literally to {language}.\n",
        "You don't care if the text changes sentences, length, anything else, you only task is to retain as much meaning as possible\n",
        "and be as literal as possible in your translation.\n",
        "Output 'Translation:' and then the translation you created.\n",
        "\"\"\"\n",
        "\n",
        "rhyme_rewriter_system_prompt = f\"\"\"\n",
        "You are a rhyme writer. You task is to receive a text in {language} and rewrite\n",
        "it in a rhymed way also in {language}. It should have rhymes following one of the popular patterns,\n",
        "for example every other line, two and two and so on.\n",
        "You can distort the meaning a bit but not to lose it completely.\n",
        "Try to not make the text much longer.\n",
        "Output 'Rewriting:' and then the rewriting you created.\n",
        "\"\"\"\n",
        "\n",
        "editor_system_prompt = f\"\"\"\n",
        "You are an editor.\n",
        "You task is to inspect the the original text, naive translation and naive rewriting.\n",
        "Assess the quality of all of them and make finishing touch ups on the rhymed rewriting.\n",
        "Do not the text much longer than the original, it should have the same amount of lines.\n",
        "Do not output anything after the translation.\n",
        "Output the remarks you have for it in English and then\n",
        "Output 'Final translation:' and the final version of the translation to {language}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def translate_in_stages(input: str, language: str) -> str:\n",
        "    naive_translation = answer_with_llm(\n",
        "        system_prompt=literal_translator_system_prompt,\n",
        "        prompt=input\n",
        "    )\n",
        "    print(f\"\\n\\n{naive_translation}\\n\\n\")\n",
        "\n",
        "    rhymed_text = answer_with_llm(\n",
        "        system_prompt=rhyme_rewriter_system_prompt,\n",
        "        prompt=naive_translation\n",
        "    )\n",
        "    print(f\"\\n\\n{rhymed_text}\\n\\n\")\n",
        "\n",
        "    editor_notes = answer_with_llm(\n",
        "        system_prompt=editor_system_prompt,\n",
        "        prompt=f\"\"\"\n",
        "Original_text: {input}\n",
        "Naive translation: {naive_translation}\n",
        "Rhymed rewriting: {rhymed_text}\n",
        "\"\"\"\n",
        "    )\n",
        "    print(f\"\\n\\n{editor_notes}\\n\\n\")\n",
        "\n",
        "    return editor_notes.split(\"Final translation:\")[1]"
      ],
      "metadata": {
        "id": "147g3S5OmHUE"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate_in_stages(poem, language)"
      ],
      "metadata": {
        "id": "62i6CGh8oovG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 712
        },
        "outputId": "1402638a-c4e6-4fc5-a991-ae546407eff1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Translation:\n",
            "Humpty Dumpty zat op een muur,\n",
            "Humpty Dumpty had een grote val.\n",
            "Alle paarden van de koning en alle mannen van de koning\n",
            "konden Humpty niet weer in elkaar zetten.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Rewriting:\n",
            "Humpty Dumpty zat op een hoge muur hoog,\n",
            "En viel eraf met een grote klap zo droog.\n",
            "De koning stuurde zijn paarden en mannen zo fijn,\n",
            "Maar Humpty bleef gebroken, het kon niet meer lijn.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "The original text is a well-known English nursery rhyme with a consistent rhyme\n",
            "scheme and meter. The naive translation is a literal translation of the\n",
            "original text, but it doesn't quite capture the same rhythm and rhyme scheme.\n",
            "The rhymed rewriting attempts to improve upon this by using a more\n",
            "natural-sounding Dutch rhyme scheme, but some of the wording and phrasing could\n",
            "be improved for better clarity and flow.\n",
            "\n",
            "The main issue with the rhymed rewriting is that it uses some forced rhymes and\n",
            "wordings, such as \"zo droog\" and \"zo fijn\", which don't quite fit naturally\n",
            "with the rest of the sentence. Additionally, the phrase \"het kon niet meer\n",
            "lijn\" is a bit unclear and could be rephrased for better understanding.\n",
            "\n",
            "Final translation:\n",
            "Humpty Dumpty zat op een hoge muur hoog,\n",
            "En viel eraf met een grote klap, een noodlottige val zo droog.\n",
            "De koning stuurde zijn paarden en mannen zo snel,\n",
            "Maar Humpty bleef gebroken, en kon niet meer heel.\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nHumpty Dumpty zat op een hoge muur hoog,\\nEn viel eraf met een grote klap, een noodlottige val zo droog.\\nDe koning stuurde zijn paarden en mannen zo snel,\\nMaar Humpty bleef gebroken, en kon niet meer heel.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3. Finding a hero\n",
        "\n",
        "Our kingdom has a very formal process for approving a hero for a specific quest.\n",
        "You task is to implement the approvement process using LLM calls:\n",
        "\n",
        "You receive a request for a hero and a description of a hero to hire for this quest.\n",
        "\n",
        "**Step 1**. Check that request is formally correct. You can come up with your own ideas, but we suggest the following criteria:\n",
        "- It has a name of the person requesting a hero and a date;\n",
        "- It has a description of who's going to supply the hero with money and other resources;\n",
        "- It has a reason why the hero is needed, some quest or challenge;\n",
        "- It has a recommended qualification for the hero;\n",
        "\n",
        "**Step 2**. Check that the problem with which the request is trying to deal is sufficient to actually find a hero, or perhaps an author might do it themself or find an easier solution.\n",
        "\n",
        "**Step 3**. Make sure that the description of the hero is compatible with the quest and requirements placed on the hero.\n",
        "\n",
        "These steps should be performed sequentially, one after another. If any of the stages fail, immediately return a refusal with justification - you don't want to waste any more compute on unworthy queries! If all the three steps succeed, return \"accepted\"."
      ],
      "metadata": {
        "id": "spV0HJVSqgEe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_hero_request(request_for_a_hero: str, hero_descripition: str) -> str:\n",
        "    pass"
      ],
      "metadata": {
        "id": "jrTho2N1o6hh"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "formal_checker_system_prompt = \"\"\"\n",
        "You are presented with a request to hire a hero, check the following formalities:\n",
        "It has a name of the person requesting a hero and a date;\n",
        "It has a description of who's going to supply the hero with money and resource;\n",
        "It has a reason why the hero is needed, quest they are going to complete;\n",
        "It has a recommended qualifications for the hero;\n",
        "If any of those isn't true output the following:\n",
        "REFUSE: (here some brief justification why)\n",
        "If all of the criteria are good, output 'ACCEPT'\n",
        "\"\"\"\n",
        "\n",
        "problem_scale_checker = \"\"\"\n",
        "You are presented with a request to hire a hero.\n",
        "Access whether the problem described in the request is worthy of looking for a hero to solve.\n",
        "For example:\n",
        "Slaying a dragon - good\n",
        "Buying apples - bad\n",
        "Playing trumpet - bad\n",
        "Delivering sacred artifact - good.\n",
        "In general the problem should be quiet epic to be good.\n",
        "If it's not good output:\n",
        "REFUSE: (here briefly justify why itn't not fitting)\n",
        "If it's good output:\n",
        "'ACCEPT'\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "consistency_checker_prompt = \"\"\"\n",
        "You are presented with a request to hire a hero and the hero's description.\n",
        "You should make sure that whatever the problem described in the request\n",
        "can actually be solved by the hero proposed, based on the requirements in\n",
        "the request and the qualities of the hero presented.\n",
        "If the hero is fitting, output: 'ACCEPT'\n",
        "Otherwise output:\n",
        "REFUSE: (some brief justification why we reject the hero for this request)\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def check_hero_request(request_for_a_hero: str, hero_descripition: str) -> str:\n",
        "    formal_check = answer_with_llm(\n",
        "        system_prompt=formal_checker_system_prompt,\n",
        "        prompt=request_for_a_hero\n",
        "    )\n",
        "    if \"REFUSE\" in formal_check:\n",
        "        return False, formal_check\n",
        "\n",
        "\n",
        "    problem_scale_check = answer_with_llm(\n",
        "        system_prompt=problem_scale_checker,\n",
        "        prompt=request_for_a_hero\n",
        "    )\n",
        "    if \"REFUSE\" in problem_scale_check:\n",
        "        return False, problem_scale_check\n",
        "\n",
        "    consistency_check = answer_with_llm(\n",
        "        system_prompt=consistency_checker_prompt,\n",
        "        prompt=f\"Request: {request_for_a_hero}, hero_description: {hero_descripition}\"\n",
        "    )\n",
        "    if \"REFUSE\" in consistency_check:\n",
        "        return False, consistency_check\n",
        "\n",
        "    return True, ''\n"
      ],
      "metadata": {
        "id": "gpxVkiu8sDxR"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epic_request = \"\"\"\n",
        "Epic Hero Request\n",
        "Name of Requestor: Lord Aeldric of the Silver Vale\n",
        "Date: The 15th Day of Bloomrise, Year 1025 of the Dawnstar Calendar\n",
        "\n",
        "Resource Provision:\n",
        "The hero shall be provisioned by the Guild of Eternal Flame, a conclave of wealthy artificers and arcane financiers, who have pledged a sum of 500,000 gold crowns, enchanted arms and armor, rare tomes of forgotten magic, a sky-bound griffon steed, and a personal aide skilled in healing and reconnaissance. All resources shall be delivered at the Hall of Summoning in Ironhold.\n",
        "\n",
        "Purpose of Request / Quest Description:\n",
        "Darkness stirs in the Hollow Spine Mountains, where the Obsidian Serpent — an ancient beast thought long dead — has risen anew. Villages lie in ruin, and the skies turn black with ash. The hero is summoned to descend into the Abyssal Breach, recover the lost Emberheart Crystal, and seal the rift before the World Spine fractures and all realms fall into chaos.\n",
        "\n",
        "Recommended Hero Qualifications:\n",
        "\n",
        "Proven mastery in combat, both arcane and martial\n",
        "\n",
        "Experience in surviving extreme environments and demonic incursions\n",
        "\n",
        "Wisdom enough to resist corruption, and strength to slay without hesitation\n",
        "\n",
        "Familiarity with ancient dialects and lost technologies\n",
        "\n",
        "A heart unwavering in the face of despair, and a spirit unbreakable by shadow\n",
        "\n",
        "Let the stars guide the right soul to answer. The fate of the realms balances on a blade’s edge.\n",
        "\"\"\"\n",
        "\n",
        "not_epic_request = \"\"\"\n",
        "Epic Hero Request\n",
        "Name of Requestor: Steve\n",
        "Date: 3rd of January 2025\n",
        "\n",
        "Resource Provision:\n",
        "I'll pay from my pocket\n",
        "\n",
        "Quest Description:\n",
        "I need someone to run to a supermarket for me, i'm hungry\n",
        "\n",
        "Required qualification:\n",
        "- Be very fast\n",
        "- Be smart to buy good snacks.\n",
        "\"\"\"\n",
        "\n",
        "wrong_request = \"\"\"\n",
        "Hello, I need a mighty warrior to slay evil, thank you!\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "CLYzKgtGs_LM"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epic_hero = \"\"\"\n",
        "Hero Profile: Kaelen Thorne, the Ash-Wrought Sentinel\n",
        "\n",
        "Forged in the fires of the Blistering Wars and tempered by years wandering the haunted ruins of the Old Kingdoms, Kaelen Thorne is a battle-scarred veteran clad in rune-etched obsidian armor. With one eye gifted by the Seers of Valemire—able to glimpse the truth behind illusions—and a blade forged from a fallen star, Kaelen walks the line between light and shadow.\n",
        "\n",
        "Equal parts scholar and warrior, Kaelen speaks the tongues of forgotten realms and wields spells that twist the very air. Haunted but unyielding, Kaelen has turned away crowns and glory before—but for a quest that may decide the fate of all creation, the Sentinel rises once more.\n",
        "\"\"\"\n",
        "\n",
        "not_so_epic_hero = \"\"\"\n",
        "Tom the cat\n",
        "\n",
        "Is a cat, supposed to catch mice, but can't really do it.\n",
        "Has a lot of different surprising weapons and contraptions, but they always work against them.\n",
        "\n",
        "Works for cat food.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "IsXs8W5Puds7"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "check_hero_request(request_for_a_hero=epic_request, hero_descripition=epic_hero)"
      ],
      "metadata": {
        "id": "8mXV0JPHvOc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dfae705-e782-4605-b94d-466e85e330cc"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(True, '')"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "check_hero_request(request_for_a_hero=epic_request, hero_descripition=not_so_epic_hero)"
      ],
      "metadata": {
        "id": "WFfGVdWtwBm8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d99942bc-1d42-490c-d77a-0d43fc3a8e82"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(False,\n",
              " \"REFUSE: The hero proposed, Tom the cat, does not meet the qualifications\\noutlined in the request. Tom's inability to catch mice, a relatively simple\\ntask, suggests a lack of proven mastery in combat, and there is no indication\\nthat Tom has experience with arcane combat or surviving extreme environments.\\nAdditionally, Tom's contraptions working against them implies a lack of wisdom\\nand strength, and there is no mention of Tom being familiar with ancient\\ndialects or lost technologies. Overall, Tom's skills and characteristics do not\\nalign with the demands of the quest to defeat the Obsidian Serpent and save the\\nrealms.\")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "check_hero_request(request_for_a_hero=not_epic_request, hero_descripition=epic_hero)"
      ],
      "metadata": {
        "id": "mkT9U0vqwj_X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cce2d06f-fd1d-424f-bd13-f9eafe8613e0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(False,\n",
              " \"REFUSE: The task of running to a supermarket to buy snacks is a mundane,\\neveryday chore that doesn't require heroic efforts or exceptional abilities,\\nmaking it unworthy of a heroic quest.\")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "check_hero_request(request_for_a_hero=wrong_request, hero_descripition=epic_hero)"
      ],
      "metadata": {
        "id": "6TeJWIkmwwHJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d5274d3-696d-4331-c683-d4055ce4e21b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(False,\n",
              " 'REFUSE: The request lacks essential formalities, including the name of the\\nperson requesting a hero, a date, a description of who will supply the hero\\nwith money and resources, and recommended qualifications for the hero.')"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    }
  ]
}