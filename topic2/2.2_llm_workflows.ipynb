{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexUmnov/LLM-Engineering-Essentials/blob/main/topic2/2.2_llm_workflows.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2Adag0lHGp5"
      },
      "source": [
        "# LLM Engineering Essentials by Nebius Academy\n",
        "\n",
        "Course github: [link](https://github.com/Nebius-Academy/LLM-Engineering-Essentials/tree/main)\n",
        "\n",
        "The course is in development now, with more materials coming soon. [Subscribe to stay updated](https://academy.nebius.com/llm-engineering-essentials/update/)\n",
        "\n",
        "# 2.2 LLM Workflows\n",
        "\n",
        "In Topic 1 we mostly studied how to get value from one LLM working in a single-call or a chat mode. But it's only a beginning! So much more may be achieved by orchestrating a complex workflow combining several LLM calls, tools, etc.\n",
        "\n",
        "Orchestration will be the core idea of Topic 2. We'll guide you through:\n",
        "\n",
        "* **LLM workflows** - **manual orchestration of several LLM calls inside one system** - in this notebook\n",
        "* Orchestrated and native LLM reasoning processes in notebooks **2.3-5**\n",
        "* Native tool usage and LLM agent basics in **2.6**\n",
        "* LLM-powered planning and agentic systems in **2.7**\n",
        "\n",
        "So, let's start this exciting journey!\n",
        "\n",
        "In this notebooks, we'll discuss how to combine LLM calls in meaningful and flexible ways. We'll mostly follow the [Building Effective Agents](https://www.anthropic.com/engineering/building-effective-agents) article by Anthropic in its workflow classification - and we really recommend you to browse through it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFWGxepmhVLh"
      },
      "source": [
        "## Getting things ready"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3UBYmNgE90I"
      },
      "outputs": [],
      "source": [
        "!pip install openai -qU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GeFeBF0mKYVL"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "import os\n",
        "\n",
        "os.environ['NEBIUS_API_KEY'] = userdata.get(\"nebius_api_key\")\n",
        "\n",
        "nebius_client = OpenAI(\n",
        "    base_url=\"https://api.studio.nebius.ai/v1/\",\n",
        "    api_key=os.environ.get(\"NEBIUS_API_KEY\"),\n",
        ")\n",
        "\n",
        "llama_model = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
        "\n",
        "def prettify_string(text, max_line_length=80):\n",
        "    \"\"\"Prints a string with line breaks at spaces to prevent horizontal scrolling.\n",
        "\n",
        "    Args:\n",
        "        text: The string to print.\n",
        "        max_line_length: The maximum length of each line.\n",
        "    \"\"\"\n",
        "\n",
        "    output_lines = []\n",
        "    lines = text.split(\"\\n\")\n",
        "    for line in lines:\n",
        "        current_line = \"\"\n",
        "        words = line.split()\n",
        "        for word in words:\n",
        "            if len(current_line) + len(word) + 1 <= max_line_length:\n",
        "                current_line += word + \" \"\n",
        "            else:\n",
        "                output_lines.append(current_line.strip())\n",
        "                current_line = word + \" \"\n",
        "        output_lines.append(current_line.strip())  # Append the last line\n",
        "    return \"\\n\".join(output_lines)\n",
        "\n",
        "def answer_with_llm(prompt: str,\n",
        "                    system_prompt=\"You are a helpful assistant\",\n",
        "                    max_tokens=512,\n",
        "                    client=nebius_client,\n",
        "                    model=llama_model,\n",
        "                    prettify=True,\n",
        "                    temperature=None) -> str:\n",
        "\n",
        "    messages = []\n",
        "\n",
        "    if system_prompt:\n",
        "        messages.append(\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": system_prompt\n",
        "            }\n",
        "        )\n",
        "\n",
        "    messages.append(\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt\n",
        "        }\n",
        "    )\n",
        "\n",
        "    completion = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        max_tokens=max_tokens,\n",
        "        temperature=temperature\n",
        "    )\n",
        "\n",
        "    if prettify:\n",
        "        return prettify_string(completion.choices[0].message.content)\n",
        "    else:\n",
        "        return completion.choices[0].message.content\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gYo9JE6jstU"
      },
      "source": [
        "# Understanding LLM workflows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QII9FvTsoD6v"
      },
      "source": [
        "## Chaining\n",
        "\n",
        "The most basic LLM workflow type is **chaining**: using several LLM calls in a sequence, the next one modifying or refining the previous ones.\n",
        "\n",
        "<center>\n",
        "<img src=\"\n",
        "https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F7418719e3dab222dccb379b8879e1dc08ad34c78-2401x1000.png&w=3840&q=75\" width=1000 />\n",
        "\n",
        "[Source](https://www.anthropic.com/engineering/building-effective-agents)\n",
        "</center>\n",
        "\n",
        "Example use cases might include:\n",
        "\n",
        "* **Localization**. Though LLMs develop towards multiliguality, it's still easier for them to answer complex instructions in English (because most of the training data, as most of web and books, is in English). A natural way of dealing with that is making a **chain**:\n",
        "\n",
        "  * The first LLM call translates the query from the source language into English\n",
        "  * The second one processes the query in English\n",
        "  * The third one translates the answer back into the source language.\n",
        "\n",
        "Before LLMs became good at structured outputs, another popular use case for chaining was (Answering the question) -> (Extracting the answer).\n",
        "\n",
        "In many cases, however, the workflows arent' sequential, so let's discuss several more comlplex types."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fQIfRbPoCOg"
      },
      "source": [
        "## Parallelization\n",
        "\n",
        "**Parallelization** is the workflow type where several workers process the query and their outputs are put together by an agregator to produce the final answer.\n",
        "\n",
        "<center>\n",
        "<img src=\"\n",
        "https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F406bb032ca007fd1624f261af717d70e6ca86286-2401x1000.png&w=3840&q=75\" width=1000 />\n",
        "\n",
        "[Source](https://www.anthropic.com/engineering/building-effective-agents)\n",
        "</center>\n",
        "\n",
        "A special case of parallelization is what can be called **LLM MapReduce**. As an example, let's consider long document summarization. if the input is too large to be processed efficiently in one call, we can\n",
        "\n",
        "* distribute input chunks between identical LLM workers (**map** phase)\n",
        "* then ask another LLM to put summaries of individual chunks together (**recude** phase)\n",
        "\n",
        "Another example might be **evaluation of chatbot conversations**. Usually, you want to evaluate your chatbot's proficiency along several axes: helpfulness, tone of voice etc - and all this can be scored by **LLM-as-a-Judge**. And generally it might be a good idea to score different parameters in different and parallel LLM calls - this way the prompts will be simpler and the judges' outputs more reliable. In such a system, Aggregator is optional; you can just put all the scores together without any additional LLMs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-yy85QCPxcf"
      },
      "source": [
        "## Routing\n",
        "\n",
        "A customer support chatbot may have a complex, tree-like logic switching a user between several conversation branches. You might just prompt one LLM thoroughy and let it rule it out in a chat mode, but if you can describe all the scenarios, why not make things more reliable by creating a **routing** workflow?\n",
        "\n",
        "<center>\n",
        "<img src=\"\n",
        "https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F5c0c0e9fe4def0b584c04d37849941da55e5e71c-2401x1000.png&w=3840&q=75\" width=1000 />\n",
        "\n",
        "[Source](https://www.anthropic.com/engineering/building-effective-agents)\n",
        "</center>\n",
        "\n",
        "In the most basic implementation, the entry-point LLM chooses between several prescribed scenarios based on the user's request. But the workflow might as well be a more complicated one:\n",
        "\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1QoIL5j2C6U5u2gE_jGcF-w3e1qlgWL7d\" width=600 />\n",
        "</center>\n",
        "\n",
        "Not all of the links must be other LLMs. Some might be rule-based processors or even involve human support specialists jumping in to help the client.\n",
        "\n",
        "Another possible application of routing is choosing between a number of LLMs of various capability. For example, you could use a 8B model for simple questions or 70B model for more elaborate ones. Classifying the question's complexity might be a job for an LLM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74CyuxOtXRGv"
      },
      "source": [
        "## Feedback loops\n",
        "\n",
        "In complex tasks we might expect that an LLM workflow wouldn't immediately arrive at the final solution. A good example is coding: the first solution may be flawed, and one or two rounds of self-analysis and self-correction might help.\n",
        "\n",
        "If you can describe the evaluation criteria, you can construct a **feedback loop** that would run until the evaluator gives the solution a pass or until the system hits max number of iterations.\n",
        "\n",
        "<center>\n",
        "<img src=\"\n",
        "https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F14f51e6406ccb29e695da48b17017e899a6119c7-2401x1000.png&w=3840&q=75\" width=1000 />\n",
        "\n",
        "[Source](https://www.anthropic.com/engineering/building-effective-agents)\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHwMtv8lO6K0"
      },
      "source": [
        "## Creating more complex workflows. Workflows vs agents\n",
        "\n",
        "From these four primitives, you can assemble workflows of arbitrary complexity. For example, here is a potential advertisement creation workflow:\n",
        "\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=14DDGHYOojUClbcI59WRn5lZ_4rAWHpoW\" width=600 />\n",
        "</center>\n",
        "\n",
        "LLM workflows are all human-designed. To create one, you need to come up with the process nodes and connections between them. At times, you would want something - an LLM! - to orchestrate everything for you.\n",
        "\n",
        "Like this:\n",
        "\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=102GUavLMfYjqR0SftsQa5VItH2TEA0JS\" width=400 />\n",
        "</center>\n",
        "\n",
        "LLM-powered orchestration makes the system into an **LLM Agent**. It's a cool and powerful thing, and we'll discuss it in more details in notebooks [A.1](https://colab.research.google.com/github/Nebius-Academy/LLM-Engineering-Essentials/blob/main/topic2/a.1_llm_tools_and_agents.ipynb) and [A.2](https://colab.research.google.com/github/Nebius-Academy/LLM-Engineering-Essentials/blob/main/topic2/a.1_llm_tools_and_agents.ipynb). However, it makes things less transparent and less reliable in comparison with manually orchestrated pipelines.\n",
        "\n",
        "In the rest of this notebook, we'll work out several particular examples of LLM orchestration: summarization and localization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5OD8m0mrMTU"
      },
      "source": [
        "# LLM workflow examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpSCGMNYBjk-"
      },
      "source": [
        "## Summarization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aD14FCgDEBkw"
      },
      "source": [
        "Let's try to write a simple LLM summarization script. As an example text we'll take an article from Wikipedia about paws.\n",
        "\n",
        "Note: we'll take a different model here, specifically **deepseek-ai/DeepSeek-R1** because it's tokenizer doesn't have additional license requirements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnneaYB5MrsO"
      },
      "outputs": [],
      "source": [
        "import requests, bs4\n",
        "\n",
        "content = requests.get(\"https://en.wikipedia.org/wiki/Paw\").content\n",
        "parsed = bs4.BeautifulSoup(content)\n",
        "content_div = parsed.find(\"div\", \"mw-content-container\")\n",
        "full_text = content_div.get_text()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxwnjZO5FUx4"
      },
      "source": [
        "We are using **BeautifulSoup** to parse out the contents of the page omitting everything except for the main text.\n",
        "\n",
        "Now let's write a simple llm summarization code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OGCE-6cREIcz"
      },
      "outputs": [],
      "source": [
        "model = \"deepseek-ai/DeepSeek-R1\"\n",
        "\n",
        "def summarize_with_llm(text):\n",
        "    chat_completion = nebius_client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[{\"role\": \"user\", \"content\": f\"Summarize the most important aspects of the following text: {text}. Try to be short.\"}]\n",
        "    )\n",
        "    return chat_completion.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgZUmZNBH3yG"
      },
      "outputs": [],
      "source": [
        "summarize_with_llm(full_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6YYJwhKIdF0"
      },
      "source": [
        "This looks like a good summary of the article. However, this is the simplest example. Let's try to do something a bit more complicated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgUoE7E1Ijzn"
      },
      "source": [
        "## Map reduce summarization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lf-fN9YnH6a6"
      },
      "outputs": [],
      "source": [
        "def get_wiki_text(url):\n",
        "    content = requests.get(url).content\n",
        "    parsed = bs4.BeautifulSoup(content)\n",
        "    content_div = parsed.find(\"div\", \"mw-content-container\")\n",
        "    full_text = content_div.get_text()\n",
        "    return full_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKgKsLA9JA7H"
      },
      "source": [
        "A page on *2023 in American television* is considered one of the longest pages on Wikipedia. It's mostly long because it's a list of all shows released that year with descriptions. However, it's great for us to test our long text summarization skills."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQgIfjREIrXI"
      },
      "outputs": [],
      "source": [
        "full_text = get_wiki_text(\"https://en.wikipedia.org/wiki/2023_in_American_television\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pp-67kasJRHe"
      },
      "source": [
        "The length of the text is not that interesting to us as the number of tokens. Let's try to look at both.\n",
        "\n",
        "We'll use **huggingface** to get the model's tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TyFgRnQJlU3"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "\n",
        "def get_token_count(text):\n",
        "    encoding = AutoTokenizer.from_pretrained(model)\n",
        "\n",
        "    encoded = encoding.encode(text)\n",
        "    return len(encoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4euitvkoJz3o"
      },
      "outputs": [],
      "source": [
        "print(f\"Number of tokens: {get_token_count(full_text)}\")\n",
        "print(f\"Number of characters: {len(full_text)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiSEf3CKKEYO"
      },
      "source": [
        "Even though technically DeepSeek R1 can take this whole text at once (it has 164000 token-long context window), it's usually not ideal. Especially because for models computations grow more than linearly proportionally to length. So it's better to summarize in **MapReduce** style, where we summarize small parts and then generate a summary for the whole text. It can also help us summarize texts which are larger than our context window.\n",
        "\n",
        "Let's experiment with both:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOQDGlMDJ2Fg"
      },
      "outputs": [],
      "source": [
        "naive_summary = summarize_with_llm(full_text)\n",
        "naive_summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRpBT5E9LA2K"
      },
      "source": [
        "To orchestrate a MapReduce pipeline, we'll need to break text down into chunks - and we don't want it to be sliced mid-sentence. So, we'll use special tools for that.\n",
        "\n",
        "Langchain has a handy tool called `TextSplitter` which allows to split a text following specific rules.\n",
        "\n",
        "For example, `RecursiveCharacterTextSplitter` can split texts recursively based on list of characters until it reaches desired length. It also allows you to set up overlap so that chunks have some connections between each other. It's a useful thing, but we will not be using this here.\n",
        "\n",
        "The default delimiter list is `[\"\\n\\n\", \"\\n\", \" \", \"\"]`. Which in theory gives us splitting by paragraphs, subparagraphs, words and then characters.\n",
        "\n",
        "Langchain also allows you to define length functions for Splitters, which will be used to determine if the chunk is of an appropriate length. We can even instantiate a length function from `tiktoken` encoder directly, so that our chunk length is tied to the token count."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZvdvqoCOXJd"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNfh3NYpKTfn"
      },
      "outputs": [],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
        "    tokenizer=AutoTokenizer.from_pretrained(model),\n",
        "    chunk_size=10000,\n",
        "    chunk_overlap=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GbvfqFoLRN9"
      },
      "outputs": [],
      "source": [
        "splitted_text = splitter.split_text(full_text)\n",
        "len(splitted_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQRYowhRNDqY"
      },
      "source": [
        "Let's create our Map and Reduce operations.\n",
        "\n",
        "Notice that langchain uses a bit of a different notation for it's chains."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPZBpVc4Mltc"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model=llama_model,\n",
        "    base_url=\"https://api.studio.nebius.ai/v1/\",\n",
        "    api_key=os.environ['NEBIUS_API_KEY']\n",
        ")\n",
        "\n",
        "map_prompt = ChatPromptTemplate.from_messages(\n",
        "    [(\"human\", \"Write a concise summary of the following:\\\\n\\\\n{context}\")]\n",
        ")\n",
        "\n",
        "map_chain = map_prompt | llm | StrOutputParser()\n",
        "\n",
        "\n",
        "reduce_template = \"\"\"\n",
        "The following is a set of summaries:\n",
        "{docs}\n",
        "Take these and distill it into a final, consolidated summary\n",
        "of the main themes.\n",
        "\"\"\"\n",
        "\n",
        "reduce_prompt = ChatPromptTemplate([(\"human\", reduce_template)])\n",
        "\n",
        "reduce_chain = reduce_prompt | llm | StrOutputParser()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9GqG47tOE2t"
      },
      "source": [
        "Now, in the simplest form our MapReduce summarization would look like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjcYjF4jPGOH"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mv4L1-6oOHpC"
      },
      "outputs": [],
      "source": [
        "def map_reduce_summarization(docs):\n",
        "    summaries = map(\n",
        "        lambda doc: map_chain.invoke({\"context\": doc}),\n",
        "        tqdm(docs)\n",
        "    )\n",
        "\n",
        "    final_summary = reduce_chain.invoke({\"docs\": \"\\n\\n\".join(summaries)})\n",
        "    return final_summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydwlpynMOQk0"
      },
      "outputs": [],
      "source": [
        "map_reduce_summarization(splitted_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEAzpF6TfmjA"
      },
      "source": [
        "## Map reduce with LLM orchestration\n",
        "\n",
        "In some cases we might need a bit more complicated orchestration of MapReduce calls. Let's imagine an example scenario.\n",
        "\n",
        "We want to create a party of adventurers. We can decompose this task into multiple steps. Namely:\n",
        "\n",
        "1. Generate a rought outline of the party, amount of members, rough descriptions.\n",
        "2. For each member generate full story and skill list\n",
        "3. Gather all descriptions and generate a short story of how they came to be together.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1ScixZwB5AK9TQWrpIg5nTErT_M0Y-S9C\" width=600 />\n",
        "</center>\n",
        "\n",
        "For part 2 we can reuse our previous example from the structured generation notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9N2kpsjOSUp"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from pydantic import BaseModel\n",
        "\n",
        "class CharacterProfile(BaseModel):\n",
        "    name: str\n",
        "    age: int\n",
        "    special_skills: List[str]\n",
        "    traits: List[str]\n",
        "    character_class: str\n",
        "    origin: str\n",
        "\n",
        "def generate_character(description: str):\n",
        "    completion = nebius_client.beta.chat.completions.parse(\n",
        "        model=llama_model,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"Design a role play character based on the following\"\\\n",
        "                          f\"short description {description}\"}\n",
        "        ],\n",
        "        extra_body={\n",
        "            \"guided_json\": CharacterProfile.model_json_schema()\n",
        "        }\n",
        "    )\n",
        "\n",
        "    return CharacterProfile.model_validate_json(completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1RG3QcwgWlQ"
      },
      "outputs": [],
      "source": [
        "generate_character(\"Dwarf druid\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nY0_iuBRg8-P"
      },
      "source": [
        "Now, for steps 1 and 3:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJ1Tr5EHgypK"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def pregenerate_party():\n",
        "    json_output = nebius_client.chat.completions.create(\n",
        "        messages=[{\n",
        "            'role': 'user', \\\n",
        "            'content': 'Generate a short description for a party of adventurers.\\n'\\\n",
        "            'A party should have 3-5 adventures and a balanced classes set, i.e. '\\\n",
        "            'have at least a melee tank, a support and a damage dealer. \\n'\\\n",
        "            'Output those short descriptions in a json format as a list with the key \"party\".\\n'\\\n",
        "            'Each description should be a string with only a couple of details'\n",
        "\n",
        "        }],\n",
        "        model=llama_model,\n",
        "        response_format={\"type\": \"json_object\"}\n",
        "    ).choices[0].message.content\n",
        "    return json.loads(json_output)['party']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xqd2c_ALhvyY"
      },
      "outputs": [],
      "source": [
        "pregenerate_party()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPBKL7g6hxyz"
      },
      "outputs": [],
      "source": [
        "def generate_back_story(party_details: str):\n",
        "    return nebius_client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                'role': 'user', \\\n",
        "                'content': 'Based on the following party details generate '\\\n",
        "                'a short story of how this party came to be together.\\n'\n",
        "                f'{str(party_details)}'\n",
        "            }\n",
        "        ],\n",
        "        model=llama_model,\n",
        "    ).choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FK9kw1SIitZi"
      },
      "source": [
        "Now to put it all together in a workflow:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hL2Lzr08isa3"
      },
      "outputs": [],
      "source": [
        "def generate_party():\n",
        "    party = pregenerate_party()\n",
        "\n",
        "    character_sheets = [\n",
        "        generate_character(character)\n",
        "        for character in party\n",
        "    ]\n",
        "\n",
        "    backstory = generate_back_story(character_sheets)\n",
        "\n",
        "    character_sheets_str = \"\\n\".join([\n",
        "        str(character) for character in character_sheets\n",
        "    ])\n",
        "\n",
        "    return f\"\"\"\n",
        "Party description:\n",
        "{json.dumps(party, indent=4)}\n",
        "\n",
        "Party backstory:\n",
        "{backstory}\n",
        "\n",
        "Party members:\n",
        "{character_sheets_str}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Ol8shxnjaeD"
      },
      "outputs": [],
      "source": [
        "print(generate_party())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bdXY1qNaI9b"
      },
      "source": [
        "## LLM localization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoCJvoI9ZcA9"
      },
      "source": [
        "Even if your base LLM is better at English that at your target language, you can easily translate your outputs with another LLM call."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pncgXMZokqII"
      },
      "outputs": [],
      "source": [
        "def translate_to_language(input: str, target_language: str):\n",
        "    return nebius_client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                'role': 'user', \\\n",
        "                'content': f'Translate the following text into {target_language}:\\n{input}'\n",
        "            }\n",
        "        ],\n",
        "        model=llama_model,\n",
        "    ).choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gS5ImGWAZw0r"
      },
      "outputs": [],
      "source": [
        "party = generate_party()\n",
        "print(party)\n",
        "translated_party = translate_to_language(party, \"Spanish\")\n",
        "print(translated_party)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spWdl38glcty"
      },
      "source": [
        "Let's also try asking the LLM to generate the party in Spanish, this omitting the translation stage:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOsSyA4gZ4zq"
      },
      "outputs": [],
      "source": [
        "def pregenerate_party_in_language(target_language: str):\n",
        "    json_output = nebius_client.chat.completions.create(\n",
        "        messages=[{\n",
        "            'role': 'user', \\\n",
        "            'content': 'Generate a short description for a party of adventurers.\\n'\\\n",
        "            'A party should have 3-5 adventures and a balanced classes set, i.e. '\\\n",
        "            'have at least a melee tank, a support and a damage dealer. \\n'\\\n",
        "            'Output those short descriptions in a json format as a list with the key \"party\".\\n'\\\n",
        "            'Each description should be a string with only a couple of details.\\n'\\\n",
        "            f'Generate in {target_language}'\n",
        "        }],\n",
        "        model=llama_model,\n",
        "        response_format={\"type\": \"json_object\"}\n",
        "    ).choices[0].message.content\n",
        "    return json.loads(json_output)['party']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVcTUx1laDv9"
      },
      "outputs": [],
      "source": [
        "pregenerate_party_in_language(\"Dutch\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGUGJ7kbKGRQ"
      },
      "source": [
        "# Practice tasks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNgipjNwzFhu"
      },
      "source": [
        "##  Task 1. Character localization\n",
        "\n",
        "Let's add localization to our simple chat NPC class from Topic 1.\n",
        "\n",
        "Your task will be to implement the following localized chat pipeline:\n",
        "- The user's input is translated into English,\n",
        "- The NPC answers an English query in English (already implemented)\n",
        "- The NPC's answer is translated into the target language, and the translation is returned to the user.\n",
        "\n",
        "Here's all the code for character creation we used before. Add new code in necessary places"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdMbiABGzhA8"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict, deque\n",
        "from openai import OpenAI\n",
        "from typing import Dict, Any, Optional\n",
        "import datetime\n",
        "import string\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class NPCConfig:\n",
        "    world_description: str\n",
        "    character_description: str\n",
        "    history_size: int = 10\n",
        "    has_scratchpad: bool = False\n",
        "\n",
        "class NPCFactoryError(Exception):\n",
        "    \"\"\"Base exception class for NPC Factory errors.\"\"\"\n",
        "    pass\n",
        "\n",
        "class NPCNotFoundError(NPCFactoryError):\n",
        "    \"\"\"Raised when trying to interact with a non-existent NPC.\"\"\"\n",
        "    def __init__(self, npc_id: str):\n",
        "        self.npc_id = npc_id\n",
        "        super().__init__(f\"NPC with ID '{npc_id}' not found\")\n",
        "\n",
        "class SimpleChatNPC:\n",
        "    def __init__(self, client: OpenAI, model: str, config: NPCConfig):\n",
        "        self.client = client\n",
        "        self.model = model\n",
        "        self.config = config\n",
        "        self.chat_histories = defaultdict(lambda: deque(maxlen=config.history_size))\n",
        "\n",
        "    def get_system_message(self) -> Dict[str, str]:\n",
        "        \"\"\"Returns the system message that defines the NPC's behavior.\"\"\"\n",
        "        character_description = self.config.character_description\n",
        "\n",
        "        if self.config.has_scratchpad:\n",
        "            character_description += \"\"\"\n",
        "You can use scratchpad for thinking before you answer: whatever you output between #SCRATCHPAD and #ANSWER won't be shown to anyone.\n",
        "You start your output with #SCRATCHPAD and after you've done thinking, you #ANSWER\"\"\"\n",
        "\n",
        "        return {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": f\"\"\"WORLD SETTING: {self.config.world_description}\n",
        "###\n",
        "{character_description}\"\"\"\n",
        "        }\n",
        "\n",
        "    def chat(self, user_message: str, user_id: str) -> str:\n",
        "        \"\"\"Process a user message and return the NPC's response.\"\"\"\n",
        "        messages = [self.get_system_message()]\n",
        "\n",
        "        # Add conversation history\n",
        "        history = list(self.chat_histories[user_id])\n",
        "        if history:\n",
        "            messages.extend(history)\n",
        "\n",
        "        # Add new user message\n",
        "        user_message_dict = {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": user_message\n",
        "        }\n",
        "        self.chat_histories[user_id].append(user_message_dict)\n",
        "        messages.append(user_message_dict)\n",
        "\n",
        "        try:\n",
        "            completion = self.client.chat.completions.create(\n",
        "                model=self.model,\n",
        "                messages=messages,\n",
        "                temperature=0.6\n",
        "            )\n",
        "\n",
        "            response = completion.choices[0].message.content\n",
        "\n",
        "            # Handle scratchpad if enabled\n",
        "            response_clean = response\n",
        "            if self.config.has_scratchpad:\n",
        "                import re\n",
        "                scratchpad_match = re.search(r\"#SCRATCHPAD(:?)(.*?)#ANSWER(:?)\", response, re.DOTALL)\n",
        "                if scratchpad_match:\n",
        "                    response_clean = response[scratchpad_match.end():].strip()\n",
        "\n",
        "\n",
        "            # Store response in history, including the scratchpad\n",
        "            self.chat_histories[user_id].append({\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": response\n",
        "            })\n",
        "\n",
        "            # Return the message to the user without a scratchpad\n",
        "            return response_clean\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"Error: {str(e)}\"\n",
        "\n",
        "class NPCFactory:\n",
        "    def __init__(self, client: OpenAI, model: str):\n",
        "        self.client = client\n",
        "        self.model = model\n",
        "        self.npcs: Dict[str, SimpleChatNPC] = {}\n",
        "        self.user_ids: Dict[str, str] = {}  # username -> user_id mapping\n",
        "\n",
        "    def generate_id(self) -> str:\n",
        "        \"\"\"Generate a random unique identifier.\"\"\"\n",
        "        return ''.join(random.choice(string.ascii_letters) for _ in range(8))\n",
        "\n",
        "    def register_user(self, username: str) -> str:\n",
        "        \"\"\"Register a new user and return their unique ID.\n",
        "        If username already exists, appends a numerical suffix.\"\"\"\n",
        "        base_username = username\n",
        "        suffix = 1\n",
        "\n",
        "        # Keep trying with incremented suffixes until we find an unused name\n",
        "        while username in self.user_ids:\n",
        "            username = f\"{base_username}_{suffix}\"\n",
        "            suffix += 1\n",
        "\n",
        "        user_id = self.generate_id()\n",
        "        self.user_ids[username] = user_id\n",
        "        return user_id\n",
        "\n",
        "    def set_user_language(self, user_id: str, language: str):\n",
        "        \"\"\"Set the preferred language for a user.\"\"\"\n",
        "        ### YOUR CODE HERE\n",
        "\n",
        "    def register_npc(self, world_description: str, character_description: str,\n",
        "                     history_size: int = 10, has_scratchpad: bool = False) -> str:\n",
        "        \"\"\"Create and register a new NPC, returning its unique ID.\"\"\"\n",
        "        npc_id = self.generate_id()\n",
        "\n",
        "        config = NPCConfig(\n",
        "            world_description=world_description,\n",
        "            character_description=character_description,\n",
        "            history_size=history_size,\n",
        "            has_scratchpad=has_scratchpad\n",
        "        )\n",
        "\n",
        "        self.npcs[npc_id] = SimpleChatNPC(self.client, self.model, config)\n",
        "        return npc_id\n",
        "\n",
        "    def chat_with_npc(self, npc_id: str, user_id: str, message: str) -> str:\n",
        "        \"\"\"Send a message to a specific NPC from a specific user.\n",
        "\n",
        "        Args:\n",
        "            npc_id: The unique identifier of the NPC\n",
        "            user_id: The unique identifier of the user\n",
        "            message: The message to send\n",
        "\n",
        "        Returns:\n",
        "            The NPC's response\n",
        "\n",
        "        Raises:\n",
        "            NPCNotFoundError: If the specified NPC doesn't exist\n",
        "        \"\"\"\n",
        "        if npc_id not in self.npcs:\n",
        "            raise NPCNotFoundError(npc_id)\n",
        "\n",
        "        npc = self.npcs[npc_id]\n",
        "        return npc.chat(message, user_id)\n",
        "\n",
        "    def get_npc_chat_history(self, npc_id: str, user_id: str) -> list:\n",
        "        \"\"\"Retrieve chat history between a specific user and NPC.\n",
        "\n",
        "        Args:\n",
        "            npc_id: The unique identifier of the NPC\n",
        "            user_id: The unique identifier of the user\n",
        "\n",
        "        Returns:\n",
        "            List of message dictionaries containing the chat history\n",
        "\n",
        "        Raises:\n",
        "            NPCNotFoundError: If the specified NPC doesn't exist\n",
        "        \"\"\"\n",
        "        if npc_id not in self.npcs:\n",
        "            raise NPCNotFoundError(npc_id)\n",
        "\n",
        "        return list(self.npcs[npc_id].chat_histories[user_id])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9za0tPtaGoQ"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "# Nebius uses the same OpenAI() class, but with additional details\n",
        "client = OpenAI(\n",
        "    base_url=\"https://api.studio.nebius.ai/v1/\",\n",
        "    api_key=os.environ.get(\"NEBIUS_API_KEY\"),\n",
        ")\n",
        "\n",
        "model = \"meta-llama/Meta-Llama-3.1-405B-Instruct\"\n",
        "\n",
        "# Creating a factory\n",
        "npc_factory = NPCFactory(client=client, model=model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjNokneB2P4k"
      },
      "outputs": [],
      "source": [
        "# Register a user\n",
        "user_id = npc_factory.register_user(\"Alice\")\n",
        "\n",
        "# Set user preffered language\n",
        "preffered_language = \"Old English\"\n",
        "npc_factory.set_user_language(user_id, preffered_language)\n",
        "\n",
        "# Create an NPC\n",
        "npc_id = npc_factory.register_npc(\n",
        "    world_description=\"Medieval London, XIII century\",\n",
        "    character_description=\"A knight at Edward I's court\",\n",
        "    has_scratchpad=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DpWeu8623GRB"
      },
      "outputs": [],
      "source": [
        "def prettify_string(text, max_line_length=80):\n",
        "    \"\"\"Prints a string with line breaks at spaces to prevent horizontal scrolling.\n",
        "\n",
        "    Args:\n",
        "        text: The string to print.\n",
        "        max_line_length: The maximum length of each line.\n",
        "    \"\"\"\n",
        "\n",
        "    output_lines = []\n",
        "    lines = text.split(\"\\n\")\n",
        "    for line in lines:\n",
        "        current_line = \"\"\n",
        "        words = line.split()\n",
        "        for word in words:\n",
        "            if len(current_line) + len(word) + 1 <= max_line_length:\n",
        "                current_line += word + \" \"\n",
        "            else:\n",
        "                output_lines.append(current_line.strip())\n",
        "                current_line = word + \" \"\n",
        "        output_lines.append(current_line.strip())  # Append the last line\n",
        "    return \"\\n\".join(output_lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCcOYeRP3MG2"
      },
      "outputs": [],
      "source": [
        "# We can hack our own code a bit and use translation features to test the system.\n",
        "\n",
        "npc = npc_factory.npcs[npc_id]\n",
        "\n",
        "message = \"Hello, who are you and what brings you here?\"\n",
        "message_translated = # use localization feature of your NPC to translate the message\n",
        "print(f\"Translated message: {message_translated}\")\n",
        "\n",
        "response = npc_factory.chat_with_npc(\n",
        "    npc_id,\n",
        "    user_id,\n",
        "    message_translated\n",
        ")\n",
        "print(\"Original answer\")\n",
        "print(prettify_string(response))\n",
        "print(\"Translated answer\")\n",
        "print(prettify_string(npc.localize_input(response)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEGu6PFqjlrk"
      },
      "source": [
        "## Task 2. Translating poetry\n",
        "\n",
        "LLMs are notoriously bad at translating poetry. The resulting poems rarely have good rhyme and rhytm. Let's try to naively translate some Humpty Dumpty rhyme to a language of your choice:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "od4336JzjxYv"
      },
      "outputs": [],
      "source": [
        "language = \"Dutch\"\n",
        "poem = \"\"\"Humpty Dumpty sat on a wall,\n",
        "Humpty Dumpty had a great fall.\n",
        "All the king's horses and all the king's men\n",
        "Couldn't put Humpty together again.\"\"\"\n",
        "\n",
        "print(answer_with_llm(\n",
        "    f\"Translate the following children's rhyme to {language}\\n{poem}\"\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cp832NFplDKN"
      },
      "source": [
        "This is hardly a good translation because the rhyme is completely broken.\n",
        "\n",
        "You task is to create a chain of LLM calls to do the following steps in translating a poem:\n",
        "\n",
        "1. Do a naive literal translation to preserve the meaning\n",
        "2. Rewrite the translation to retain rhyme and rhythm but perhaps loosing a bit of meaning.\n",
        "3. Finally have an editor look at both original and translation and make final touch ups.\n",
        "\n",
        "We encourage you to try and prompt your LLMs to do the job of \"Translator\", \"Editor\" and so on.\n",
        "Also choose the language you can understand best so that you can evaluate the result well (you can also change the original to some other language)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJvC7DthlG_x"
      },
      "outputs": [],
      "source": [
        "def translate_in_stages(input: str, language: str) -> str:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62i6CGh8oovG"
      },
      "outputs": [],
      "source": [
        "translate_in_stages(poem, language)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spV0HJVSqgEe"
      },
      "source": [
        "## Task 3. Finding a hero\n",
        "\n",
        "Our kingdom has a very formal process for approving a hero for a specific quest.\n",
        "You task is to implement the approvement process using LLM calls:\n",
        "\n",
        "You receive a request for a hero and a description of a hero to hire for this quest.\n",
        "\n",
        "**Step 1**. Check that request is formally correct. You can come up with your own ideas, but we suggest the following criteria:\n",
        "- It has a name of the person requesting a hero and a date;\n",
        "- It has a description of who's going to supply the hero with money and other resources;\n",
        "- It has a reason why the hero is needed, some quest or challenge;\n",
        "- It has a recommended qualification for the hero;\n",
        "\n",
        "**Step 2**. Check that the problem with which the request is trying to deal is sufficient to actually find a hero, or perhaps an author might do it themself or find an easier solution.\n",
        "\n",
        "**Step 3**. Make sure that the description of the hero is compatible with the quest and requirements placed on the hero.\n",
        "\n",
        "These steps should be performed sequentially, one after another. If any of the stages fail, immediately return a refusal with justification - you don't want to waste any more compute on unworthy queries! If all the three steps succeed, return \"accepted\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrTho2N1o6hh"
      },
      "outputs": [],
      "source": [
        "def check_hero_request(request_for_a_hero: str, hero_descripition: str) -> str:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLYzKgtGs_LM"
      },
      "outputs": [],
      "source": [
        "epic_request = \"\"\"\n",
        "Epic Hero Request\n",
        "Name of Requestor: Lord Aeldric of the Silver Vale\n",
        "Date: The 15th Day of Bloomrise, Year 1025 of the Dawnstar Calendar\n",
        "\n",
        "Resource Provision:\n",
        "The hero shall be provisioned by the Guild of Eternal Flame, a conclave of wealthy artificers and arcane financiers, who have pledged a sum of 500,000 gold crowns, enchanted arms and armor, rare tomes of forgotten magic, a sky-bound griffon steed, and a personal aide skilled in healing and reconnaissance. All resources shall be delivered at the Hall of Summoning in Ironhold.\n",
        "\n",
        "Purpose of Request / Quest Description:\n",
        "Darkness stirs in the Hollow Spine Mountains, where the Obsidian Serpent  an ancient beast thought long dead  has risen anew. Villages lie in ruin, and the skies turn black with ash. The hero is summoned to descend into the Abyssal Breach, recover the lost Emberheart Crystal, and seal the rift before the World Spine fractures and all realms fall into chaos.\n",
        "\n",
        "Recommended Hero Qualifications:\n",
        "\n",
        "Proven mastery in combat, both arcane and martial\n",
        "\n",
        "Experience in surviving extreme environments and demonic incursions\n",
        "\n",
        "Wisdom enough to resist corruption, and strength to slay without hesitation\n",
        "\n",
        "Familiarity with ancient dialects and lost technologies\n",
        "\n",
        "A heart unwavering in the face of despair, and a spirit unbreakable by shadow\n",
        "\n",
        "Let the stars guide the right soul to answer. The fate of the realms balances on a blades edge.\n",
        "\"\"\"\n",
        "\n",
        "not_epic_request = \"\"\"\n",
        "Epic Hero Request\n",
        "Name of Requestor: Steve\n",
        "Date: 3rd of January 2025\n",
        "\n",
        "Resource Provision:\n",
        "I'll pay from my pocket\n",
        "\n",
        "Quest Description:\n",
        "I need someone to run to a supermarket for me, i'm hungry\n",
        "\n",
        "Required qualification:\n",
        "- Be very fast\n",
        "- Be smart to buy good snacks.\n",
        "\"\"\"\n",
        "\n",
        "wrong_request = \"\"\"\n",
        "Hello, I need a mighty warrior to slay evil, thank you!\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IsXs8W5Puds7"
      },
      "outputs": [],
      "source": [
        "epic_hero = \"\"\"\n",
        "Hero Profile: Kaelen Thorne, the Ash-Wrought Sentinel\n",
        "\n",
        "Forged in the fires of the Blistering Wars and tempered by years wandering the haunted ruins of the Old Kingdoms, Kaelen Thorne is a battle-scarred veteran clad in rune-etched obsidian armor. With one eye gifted by the Seers of Valemireable to glimpse the truth behind illusionsand a blade forged from a fallen star, Kaelen walks the line between light and shadow.\n",
        "\n",
        "Equal parts scholar and warrior, Kaelen speaks the tongues of forgotten realms and wields spells that twist the very air. Haunted but unyielding, Kaelen has turned away crowns and glory beforebut for a quest that may decide the fate of all creation, the Sentinel rises once more.\n",
        "\"\"\"\n",
        "\n",
        "not_so_epic_hero = \"\"\"\n",
        "Tom the cat\n",
        "\n",
        "Is a cat, supposed to catch mice, but can't really do it.\n",
        "Has a lot of different surprising weapons and contraptions, but they always work against them.\n",
        "\n",
        "Works for cat food.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mXV0JPHvOc7"
      },
      "outputs": [],
      "source": [
        "check_hero_request(request_for_a_hero=epic_request, hero_descripition=epic_hero)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFfGVdWtwBm8"
      },
      "outputs": [],
      "source": [
        "check_hero_request(request_for_a_hero=epic_request, hero_descripition=not_so_epic_hero)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkT9U0vqwj_X"
      },
      "outputs": [],
      "source": [
        "check_hero_request(request_for_a_hero=not_epic_request, hero_descripition=epic_hero)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6TeJWIkmwwHJ"
      },
      "outputs": [],
      "source": [
        "check_hero_request(request_for_a_hero=wrong_request, hero_descripition=epic_hero)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
